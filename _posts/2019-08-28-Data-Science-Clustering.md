----

title: "Data Science - Clustering"

date: 2019-08-28 16:20

categories: development python data-science

------



## 1. Clustering Overview

* Clustering ê°œìš”

  ### 1-1. í´ëŸ¬ìŠ¤í„°ë§ì´ë€?

  * **í´ëŸ¬ìŠ¤í„° (Cluster)**

    * ë°ì´í„° í¬ì¸íŠ¸ë“¤(data objects)ì˜ ëª¨ì„
      - ê°™ì€ ê·¸ë£¹(or cluster) ì•ˆì—ì„œëŠ” ì„œë¡œ ë¹„ìŠ·í•˜ê³ 
      - ì„œë¡œ ë‹¤ë¥¸ ê·¸ë£¹ ì‚¬ì´ì—ëŠ” ë¹„ìŠ·í•˜ì§€ ì•Šë‹¤

  * **í´ëŸ¬ìŠ¤í„°ë§ (Clustering, cluster analysis, data segmentation)**

    - ë°ì´í„° í¬ì¸íŠ¸ë“¤ì„ í´ëŸ¬ìŠ¤í„°ë“¤ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒ
    - ì¦‰, ë°ì´í„°ì…‹ì—ì„œ í•˜ìœ„ê·¸ë£¹(subgroup)ì„ ì°¾ëŠ” ê²ƒì´ë‹¤.

  * **ë¶„ë¥˜ vs í´ëŸ¬ìŠ¤í„°ë§**

    - í´ëŸ¬ìŠ¤í„°ë§ : ë¹„ì§€ë„ í•™ìŠµ (unsupervised learning) - **ë ˆì´ë¸”ì´ ì—†ìŒ**
    - ë¶„ë¥˜ : ì§€ë„ í•™ìŠµ (supervised learning)

  * **í´ëŸ¬ìŠ¤í„°ë§ì€ ì–¸ì œ ì‚¬ìš©í•˜ë‚˜ìš”**

    - ë°ì´í„°ì—ì„œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ê¸° ìœ„í•´
      - ë°ì´í„°ì˜ ìš”ì•½
    - ë‹¤ë¥¸ ë°ì´í„° ë§ˆì´ë‹ ì‘ì—…ì˜ ì¤‘ê°„ ê³¼ì •ìœ¼ë¡œ
      - feature extraction
      - outlier detection. ì–´ë–¤ í´ëŸ¬ìŠ¤í„°ì—ì„œë„ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆëŠ” ë°ì´í„°ë“¤
    - ë°ì´í„° ì••ì¶•
      - ì´ë¯¸ì§€ ì••ì¶• ë“±
    - ì¶”ì²œ ì‹œìŠ¤í…œ
      - ë¹„ìŠ·í•œ ìœ ì €ë‚˜ ë¹„ìŠ·í•œ ì œí’ˆì„ ì°¾ê¸°
    - ë¹„ì¦ˆë‹ˆìŠ¤
      - market segmentation

  * **í´ëŸ¬ìŠ¤í„°ë§ì—ì„œ ì–´ë ¤ìš´ ì **

    - Quality

      - numerical, categorical, text, multimedia, networks ë“± ë‹¤ì–‘í•œ í˜•ì‹ì˜ ì†ì„±ë“¤ ì²˜ë¦¬
      - ë¶ˆê·œì¹™í•œ ëª¨ì–‘ì˜ í´ëŸ¬ìŠ¤í„° ë°œê²¬í•˜ê¸°
      - ë…¸ì´ì¦ˆê°€ ìˆëŠ” ë°ì´í„° ë‹¤ë£¨ê¸°

    - Scalability

      - ìƒ˜í”Œì´ ì•„ë‹Œ ëª¨ë“  ë°ì´í„° í´ëŸ¬ìŠ¤í„°ë§ í•˜ê¸°
      - ë†’ì€ ì°¨ì› ë‹¤ë£¨ê¸°
      - ì ì§„ì (incremental or stream) ì…ë ¥ ë‹¤ë£¨ê¸°. ì…ë ¥ ìˆœì„œì— ë¯¼ê°í•˜ì§€ ì•Šê¸°

    - Constraint-based clustering

      - User-given preferences or constraints

    - í•´ì„ê°€ëŠ¥ì„±ê³¼ ì‚¬ìš©ê°€ëŠ¥ì„±(interpretability and usability)

    - **ì¬í˜„ì„±(reproducability)**

      

  ### 1-2. í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë²•ë“¤ (Clustering methodologies)

  * K-means clustering

    - ë¯¸ë¦¬ ì •í•´ì§„ ìˆ«ìì˜ í´ëŸ¬ìŠ¤í„°ë¡œ í´ëŸ¬ìŠ¤í„°ë§ì„ í•œë‹¤.

  * Hierarchical clustering *<- ì´ê²Œ ì¡°ê¸ˆ ë” ë‚«ë‹¤*

    - ëª‡ ê°œì˜ í´ëŸ¬ìŠ¤í„°ë¥¼ ì›í•˜ëŠ”ì§€ ëª¨ë¥¼ ë•Œ
    - ë°ì´í„°ê°€ dendrogramì´ë¼ëŠ” íŠ¸ë¦¬ ëª¨ì–‘ìœ¼ë¡œ ë‚˜íƒ€ë‚´ì§„ë‹¤.
    - ê·¸ ë’¤ ëª‡ ê°œì˜ í´ëŸ¬ìŠ¤í„°ë¥¼ ì›í•˜ëŠ”ì§€ ì„ íƒí•œë‹¤.

    #### 1-2-1. K-means clustering

    * **ì•Œê³ ë¦¬ì¦˜**

      * ë¨¼ì € ì›í•˜ëŠ” í´ëŸ¬ìŠ¤í„°ì˜ ê°œìˆ˜ Kë¥¼ ê³ ë¥¸ë‹¤.
      * ê°ê°ì˜ ë°ì´í„° í¬ì¸íŠ¸ë§ˆë‹¤ ë¬´ì‘ìœ„ë¡œ 1ë¶€í„° Kê¹Œì§€ì˜ í´ëŸ¬ìŠ¤í„° ë²ˆí˜¸ë¥¼ ì§€ì •í•œë‹¤. (ì´ˆê¸°í™”)
      * í´ëŸ¬ìŠ¤í„° í• ë‹¹(assignment)ì´ ë°”ë€Œì§€ ì•Šì„ ë•Œê¹Œì§€ ë‹¤ìŒì„ ë°˜ë³µí•œë‹¤.
        - ê°ê°ì˜ í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ë°ì´í„° í¬ì¸íŠ¸ë“¤ì„ í‰ê· ë‚´ì–´, í´ëŸ¬ìŠ¤í„°ì˜ ì¤‘ì‹¬(centroid)ì„ ê³„ì‚°í•œë‹¤.
        - ê°ê°ì˜ ë°ì´í„° í¬ì¸íŠ¸ë§ˆë‹¤ ê°€ì¥ ê°€ê¹Œìš´ ì¤‘ì‹¬(centroid)ë¥¼ ê°€ì§„ í´ëŸ¬ìŠ¤í„°ì— í• ë‹¹í•œë‹¤.

      ![](D:\soyeon.kim\2. ê°œì¸\images\36.PNG)

      ![](D:\soyeon.kim\2. ê°œì¸\images\37.PNG)

    * **ì´ˆê¸°í™” ë°©ë²•**

      * ë¬´ì‘ìœ„ë¡œ ê³µê°„ ìƒì—ì„œ ì¤‘ì‹¬(centroid)ë¥¼ ì§€ì •í•˜ê¸°
      * ë¬´ì‘ìœ„ë¡œ Kê°œì˜ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ê³¨ë¼ì„œ ê°ê°ì˜ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ìœ¼ë¡œ ì§€ì •í•˜ê¸°

    * **íŠ¹ì§•**

      * ëª¨ë“  ë°ì´í„° í¬ì¸íŠ¸ëŠ” Kê°œ í´ëŸ¬ìŠ¤í„° ì¤‘ì— í•˜ë‚˜ì— ì†í•œë‹¤.
      * ì–´ë–¤ ë°ì´í„° í¬ì¸íŠ¸ë„ ë‘ ê°œ ì´ìƒì˜ í´ëŸ¬ìŠ¤í„°ì— ì†í•  ìˆ˜ ì—†ë‹¤.
      * ëœë¤ì„±ì´ ìˆë‹¤.
      * í´ëŸ¬ìŠ¤í„° ì‚¬ì´ì˜ ìˆœì„œëŠ” ì—†ë‹¤.

    * **ìˆ˜í•™ì  ì¸ì‚¬ì´íŠ¸**

      * ì¢‹ì€ í´ëŸ¬ìŠ¤í„°ë§ : í´ëŸ¬ìŠ¤í„° ë‚´ì˜ ë¶„ì‚°(within-cluster variation)ì´ ì‘ì€ í´ëŸ¬ìŠ¤í„°ë§

        ğ‘šğ‘–ğ‘›ğ‘–ğ‘šğ‘–ğ‘§ğ‘’ğ¶1,...,ğ¶ğ¾âˆ‘ğ‘˜=1ğ¾ğ‘Š(ğ¶ğ‘˜)minimizeC1,...,CKâˆ‘k=1KW(Ck)

        ğ‘Š(ğ¶ğ‘˜)=1|ğ¶ğ‘˜|âˆ‘ğ‘–,ğ‘–â€²âˆˆğ¶ğ‘˜âˆ‘ğ‘—=1ğ‘(ğ‘¥ğ‘–ğ‘—âˆ’ğ‘¥ğ‘–â€²ğ‘—)2W(Ck)=1|Ck|âˆ‘i,iâ€²âˆˆCkâˆ‘j=1p(xijâˆ’xiâ€²j)2

        - ğ‘Š(ğ¶ğ‘˜)W(Ck)ëŠ” kë²ˆì§¸ í´ëŸ¬ìŠ¤í„°ì˜ í´ëŸ¬ìŠ¤í„° ë‚´ ë¶„ì‚°(within-cluster variation)
        - |ğ¶ğ‘˜||Ck|ëŠ” kë²ˆì§¸ í´ëŸ¬ìŠ¤í„°ì— ì†í•˜ëŠ” ë°ì´í„° í¬ì¸íŠ¸ì˜ ê°œìˆ˜
        - kë²ˆì§¸ í´ëŸ¬ìŠ¤í„° ë‚´ ë¶„ì‚°ì€ kë²ˆì§¸ í´ëŸ¬ìŠ¤í„°ì— ì†í•˜ëŠ” ëª¨ë“  ë°ì´í„° í¬ì¸íŠ¸ë“¤ì˜ ìŒ ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ë”í•˜ê³  ë°ì´í„°ì˜ ê°œìˆ˜ë¡œ ë‚˜ëˆˆ ê²ƒì´ë‹¤.

    * **í´ëŸ¬ìŠ¤í„° ë‚´ ë¶„ì‚°ì„ ìµœì†Œí™”í•˜ëŠ” í´ëŸ¬ìŠ¤í„°ë§ ì°¾ê¸°**

      - ë§¤ìš° ì–´ë ¤ìš´ ë¬¸ì œì´ë‹¤.
      - nê°œì˜ ë°ì´í„°í¬ì¸íŠ¸ë¥¼ Kê°œì˜ í´ëŸ¬ìŠ¤í„°ë¡œ ë‚˜ëˆ„ëŠ” ë°ëŠ” Kì˜ nìŠ¹ ê°œì˜ ë°©ë²•ì´ ìˆê¸° ë•Œë¬¸
      - K-means ì•Œê³ ë¦¬ì¦˜ì€ local optimumì„ ì°¾ì•„ì£¼ëŠ” ê·¼ì‚¬ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.
      - ì•Œê³ ë¦¬ì¦˜ íŠ¹ì§• : ë§¤ë²ˆ iterationì„ í•  ë•Œë§ˆë‹¤, ğ‘Š(ğ¶ğ‘˜)W(Ck)ëŠ” ì¤„ì–´ë“¤ê¸°ë§Œ í•  ë¿ ì¦ê°€í•˜ì§€ ì•ŠëŠ”ë‹¤.

    * **k-means ì•Œê³ ë¦¬ì¦˜ì„ ì—¬ëŸ¬ë²ˆ í•´ì„œ ì„œë¡œ ë‹¤ë¥¸ ê²°ê³¼ë¥¼ ì–»ì—ˆë‹¤ë©´**

      - í´ëŸ¬ìŠ¤í„° ë‚´ ë¶„ì‚°ì´ ì‘ì€ ìª½ì„ ì„ íƒí•œë‹¤.

        ![](D:\soyeon.kim\2. ê°œì¸\images\38.PNG)

    

    #### 1-2-2. ê³„ì¸µí˜• í´ëŸ¬ìŠ¤í„°ë§ (Hierarchical Clustering)

    * **ì•Œê³ ë¦¬ì¦˜**

      * ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ê°ê° ë°ì´í„° 1ê°œë¥¼ ê°–ê³  ìˆëŠ” í´ëŸ¬ìŠ¤í„°ë¼ê³  ë³¸ë‹¤.
      * ë” ì´ìƒ í•  ê²Œ ì—†ì„ ë•Œê¹Œì§€ ë‹¤ìŒì„ ë°˜ë³µí•œë‹¤.
        - ì§€ê¸ˆ ìˆëŠ” ëª¨ë“  í´ëŸ¬ìŠ¤í„°ë“¤ ê°„ì˜ ê±°ë¦¬(dissimmilarities)ë¥¼ ê³„ì‚°í•œë‹¤.
        - ê°€ì¥ ê±°ë¦¬ê°€ ì‘ì€ ë‘ í´ëŸ¬ìŠ¤í„°ë¥¼ í•©ì¹œë‹¤. (ì´ ë•Œ ë‘ í´ëŸ¬ìŠ¤í„° ê°„ì˜ ê±°ë¦¬ê°€ dendrogramì—ì„œ ì„¸ë¡œì¶•ì˜ ê°’ì´ ëœë‹¤.)

      ![](D:\soyeon.kim\2. ê°œì¸\images\39.PNG)

      ![](D:\soyeon.kim\2. ê°œì¸\images\40.PNG)

    * **íŠ¹ì§•**

      * ê³„ì¸µí˜• í´ëŸ¬ìŠ¤í„°ë§ (Hierarchical clustering)ì€ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ë¥¼ ë¯¸ë¦¬ ì§€ì •í•˜ì§€ ì•Šì•„ë„ ëœë‹¤.
      * ë°ì´í„°ë¥¼ íŠ¸ë¦¬ ëª¨ì–‘ìœ¼ë¡œ ë‚˜íƒ€ë‚´ëŠ” dendrogramì„ ë§Œë“¤ì–´ë‚¸ë‹¤.

    * **Dendrogram**

      ![](D:\soyeon.kim\2. ê°œì¸\images\41.PNG)

      ![](D:\soyeon.kim\2. ê°œì¸\images\42.PNG)

    * **Dendogram í•´ì„**

      * Dendrogramì˜ 'ì'ì€ ê°ê°ì˜ ë°ì´í„° í¬ì¸íŠ¸ë“¤ì´ë‹¤.
      * íŠ¸ë¦¬ ìœ„ë¡œ ì˜¬ë¼ê°€ë©° ê°€ì¥ ê°€ê¹Œìš´ ë°ì´í„° í¬ì¸íŠ¸ë“¤ë¼ë¦¬ í•©ì³ì ¸ì„œ ê°€ì§€(branch)ë¥¼ ë§Œë“ ë‹¤.
      * ë” ìœ„ë¡œ ì˜¬ë¼ê°€ë©´ ê°€ì§€(branch)ë“¤ì´ ë‹¤ë¥¸ ì(leave)ì´ë‚˜ ê°€ì§€(branch)ì™€ í•©ì³ì§„ë‹¤.
      * ê°€ê¹Œìš´ ê°€ì§€ì¼ ìˆ˜ë¡ í•©ì³ì§€ëŠ” ê²Œ ë” ë¹¨ë¦¬ (tree ì•„ë˜ ë¶€ë¶„ì—ì„œ) ì¼ì–´ë‚œë‹¤.
      * ì¦‰, ë‚˜ì¤‘ì— (treeì˜ ìœ—ë¶€ë¶„ì—ì„œ) í•©ì³ì§ˆ ìˆ˜ë¡ ë‘ ê°€ì§€ê°€ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒ ëœ»í•œë‹¤.

    * **Dendrogram ê¸°ë°˜ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§í•˜ê¸°**

      - ìˆ˜í‰ì„  ë°©í–¥ìœ¼ë¡œ ì–´ë””ì—ì„œ ìë¥¼ì§€ ê²°ì •í•˜ë©´ í´ëŸ¬ìŠ¤í„°ë§ì´ ê²°ì •ëœë‹¤.
      - ì¦‰, ì–´ëŠ ê±°ë¦¬(distance) ì´í•˜ì˜ ê°€ì§€ê¹Œì§€ë§Œ í•©ì³ì§ˆ ìˆ˜ ìˆê²Œ í•˜ë©´ í´ëŸ¬ìŠ¤í„°ë§ì´ ê²°ì •ëœë‹¤.
      - í•˜ë‚˜ì˜ dendrogramì—ì„œ ì–´ë–¤ ê°œìˆ˜ì˜ í´ëŸ¬ìŠ¤í„°ë„ êµ¬í•  ìˆ˜ ìˆë‹¤.
      - dendrogramì„ ëˆˆìœ¼ë¡œ ë³´ê³  ê·¸ëŸ´ ë“¯í•œ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ë¥¼ ì°¾ì•„ë‚¼ ìˆ˜ ìˆëŠ” ê²ƒì´ ì¥ì ì´ë‹¤.

    * **ì™œ Hierarchical ì´ë¼ê³  í•˜ë‚˜ìš”?**

      - í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ë¥¼ ì¤„ì—¬ê°ˆ ìˆ˜ë¡, í•­ìƒ ê¸°ì¡´ í´ëŸ¬ìŠ¤í„°ê°€ í•©ì³ì ¸ì„œ ë” í° í´ëŸ¬ìŠ¤í„°ë¥¼ í˜•ì„±í•œë‹¤.

      - í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ë¥¼ ëŠ˜ì—¬ê°ˆ ìˆ˜ë¡, í•­ìƒ ê¸°ì¡´ í´ëŸ¬ìŠ¤í„°ê°€ ë‚˜ëˆ ì ¸ì„œ ë” ì‘ì€ í´ëŸ¬ìŠ¤í„°ê°€ ëœë‹¤.

        ![](D:\soyeon.kim\2. ê°œì¸\images\43.PNG)

    * **Dendrogramì„ í•´ì„í•  ë•Œ í”íˆ í•˜ëŠ” ì‹¤ìˆ˜**

      - ìœ„ ê·¸ë¦¼ì—ì„œ, 5ì™€ 7ì€ ìƒë‹¹íˆ ë¹„ìŠ·í•˜ë‹¤. (ë§ìŒ)
      - 1ê³¼ 6ë„ ìƒë‹¹íˆ ë¹„ìŠ·í•˜ë‹¤. (ë§ìŒ)
      - 9ì™€ 2ëŠ” ìƒë‹¹íˆ ë¹„ìŠ·í•˜ë‹¤. (í‹€ë¦¼)
      - 9ëŠ” 8, 5, 7ë³´ë‹¤ 2ì— ë” ê°€ê¹ì§€ ì•Šë‹¤.
      - dendrogramì„ ê·¸ë¦´ ë•Œ leaveì˜ ìˆœì„œëŠ” ì¤‘ìš”í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, ê°€ë¡œì¶• ë°©í–¥ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ì§€ëŠ” ì˜ë¯¸ê°€ ì—†ë‹¤.
      - ì˜¤ì§ ë‘ ê°€ì§€(branches)ê°€ í•©ì³ì§€ëŠ” ìˆœê°„ì˜ ì„¸ë¡œì¶•ì˜ ê°’(distance)ì´ ë‘ ê·¸ë£¹ì´ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ë¥¼ ì•Œë ¤ì£¼ëŠ” ì²™ë„ì´ë‹¤.

    * **ê±°ë¦¬ë¥¼ ì–´ë–»ê²Œ ì¸¡ì •í•  ê²ƒì¸ê°€**

      ë‘ ê°€ì§€ ê±°ë¦¬ë¥¼ ì •ì˜í•´ì•¼ í•œë‹¤.

      1. **ë‘ ë°ì´í„° í¬ì¸íŠ¸ ì‚¬ì´ì˜ ê±°ë¦¬(dissimilarity)**

         - ë³´í†µ ìœ í´ë¦¬ë“œ ê±°ë¦¬(Euclidean distance)ê°€ ì“°ì¸ë‹¤.
         - ë‹¤ë¥¸ ê±°ë¦¬ë„ ì“¸ ìˆ˜ ìˆë‹¤.(Correlation-based distance, Manhattan distance, Minkowski distance)

         ![](D:\soyeon.kim\2. ê°œì¸\images\44.PNG)

      2. **ë‘ í´ëŸ¬ìŠ¤í„°(cluster) ì‚¬ì´ì˜ ê±°ë¦¬(dissimilarity)**

         - ê±°ë¦¬(dissimilarity)ì˜ ì¼ë°˜í™”ëœ ê°œë…ì´ë‹¤.

         - linkageë¼ê³  ë¶€ë¥¸ë‹¤.

           ![](D:\soyeon.kim\2. ê°œì¸\images\45.PNG)

         - **Linkageì˜ ì¢…ë¥˜**

           - Complete

             - ìµœëŒ€ í´ëŸ¬ìŠ¤í„°ê°„ ê±°ë¦¬ (Maximal intercluster dissimilarity). A í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ì ë“¤ê³¼ B í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ì ë“¤ ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ëª¨ë‘ ê³„ì‚°í•´ì„œ, ê°€ì¥ í° ê±°ë¦¬ê°€ ë‘ í´ëŸ¬ìŠ¤í„° ì‚¬ì´ì˜ ê±°ë¦¬ê°€ ëœë‹¤.

               ![](D:\soyeon.kim\2. ê°œì¸\images\46.PNG)

           - Single

             - ìµœì†Œ í´ëŸ¬ìŠ¤í„°ê°„ ê±°ë¦¬ (Minimal intercluster dissimilarity). ê°€ì¥ ì‘ì€ ê±°ë¦¬ê°€ ë‘ í´ëŸ¬ìŠ¤í„° ì‚¬ì´ì˜ ê±°ë¦¬ê°€ ëœë‹¤. ì´ ë°©ì‹ìœ¼ë¡œ í•˜ë©´ ë³´í†µ í•œ í´ëŸ¬ìŠ¤í„°ê°€ ê³„ì†í•´ì„œ ì»¤ì§€ëŠ” ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤.

               ![](D:\soyeon.kim\2. ê°œì¸\images\47.PNG)

           - Average

             - í‰ê·  í´ëŸ¬ìŠ¤í„°ê°„ ê±°ë¦¬ (Average intercluster dissimilarity). ë‘ í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ì ë“¤ ì‚¬ì´ ê±°ë¦¬ë¥¼ êµ¬í•´ì„œ í‰ê·  ë‚¸ë‹¤.

               ![](D:\soyeon.kim\2. ê°œì¸\images\48.PNG)

           - Centroid

             - ì¤‘ì‹¬(centroid) ì‚¬ì´ì˜ ê±°ë¦¬. ì¤‘ì‹¬ì€ ê·¸ í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ì ë“¤ì˜ í‰ê· ìœ¼ë¡œ ê²°ì •ëœë‹¤.

               ![](D:\soyeon.kim\2. ê°œì¸\images\49.PNG)

           - Median

             - ë‘ í´ëŸ¬ìŠ¤í„°ê°„ ê±°ë¦¬ë¥¼ ë‘ í´ëŸ¬ìŠ¤í„°ì˜ ëª¨ë“  ë©¤ë²„ê°„ ê±°ë¦¬ì˜ ì¤‘ì•™ê°’(median)ìœ¼ë¡œ ë´…ë‹ˆë‹¤.

           - Ward

             - ë‘ í´ëŸ¬ìŠ¤í„°ê°„ ê±°ë¦¬ë¥¼ ë‘ í´ëŸ¬ìŠ¤í„°ì˜ ëª¨ë“  ë©¤ë²„ê°„ ê±°ë¦¬ì˜ í‰ê· (median)ìœ¼ë¡œ ë³´ëŠ”ë°, ë‹¨, ê³µë¶„ì‚°ì„ ê³ ë ¤í•œ ë³´ì •ì„ í•´ì¤ë‹ˆë‹¤.
             - ì´ê²ƒì€ ì›í˜•ìœ¼ë¡œ ë°ì´í„°ê°€ ë¶„ì‚°ë˜ì–´ ìˆë‹¤ë©´ euclidean distanceê°€ ì ì ˆí•˜ì§€ë§Œ íƒ€ì›í˜•ì´ë¼ë©´ variableê°„ ê³µë¶„ì‚°ì„ ê³ ë ¤í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

           - **Averageë‚˜ completeê°€ ê°€ì¥ ìì£¼ ì“°ì¸ë‹¤.**

             - ë” ê· í˜•ì¡íŒ í´ëŸ¬ìŠ¤í„°ê°€ ë§Œë“¤ì–´ì§„ë‹¤.
             - singleì€ ë³´í†µ í•œ í´ëŸ¬ìŠ¤í„°ê°€ ê³„ì† ì»¤ì§€ëŠ” ë¬¸ì œê°€ ìˆë‹¤.
             - centroidëŠ” inversionì´ë¼ëŠ” ë¬¸ì œê°€ ìˆë‹¤. ì¦‰, dendrogram ìƒì—ì„œ ë‘ í´ëŸ¬ìŠ¤í„°ê°€ ê°ê°ì˜ í´ëŸ¬ìŠ¤í„°ë³´ë‹¤ ë‚®ì€ ë†’ì´ì—ì„œ í•©ì³ì§€ëŠ” ê²ƒì´ë‹¤. dendrogramì„ ê·¸ë¦´ ë•Œ ë¬¸ì œê°€ ìƒê¸´ë‹¤.

    #### 1-2-3. K-means vs. Hierarchical clustering

    * ê³„ì¸µí˜• í´ëŸ¬ìŠ¤í„°ë§ì˜ ì¥ì 

      * í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ë¥¼ ë¯¸ë¦¬ ê²°ì •í•˜ì§€ ì•Šì•„ë„ ëœë‹¤.
      * dendrogramìœ¼ë¡œ ë°ì´í„°ë¥¼ ì‹œê°í™”í•  ìˆ˜ ìˆë‹¤.
      * ëœë¤ì„±ì´ ì—†ë‹¤.

    * K meansì˜ ì¥ì 

      * ìƒëŒ€ì ìœ¼ë¡œ ë¹ ë¥´ë‹¤.

      * ì–´ë–¤ ë°ì´í„°ì…‹ì—ì„œëŠ” í° í´ëŸ¬ìŠ¤í„°ë§ì´ ì‘ì€ í´ëŸ¬ìŠ¤í„°ë§ì„ í¬í•¨í•˜ëŠ” ê²ƒì´ ë¶€ì ì ˆí•  ìˆ˜ ìˆë‹¤.

        - ì˜ˆë¥¼ ë“¤ì–´, ë‚¨ë…€ ë¹„ìœ¨ì´ 50 ëŒ€ 50 ì´ë©° ë¯¸êµ­ì¸, ì¼ë³¸ì¸, í”„ë‘ìŠ¤ì¸ ì„¸ ì¸ì¢…ìœ¼ë¡œ ì‚¼ë“±ë¶„ ë˜ì–´ ìˆëŠ” ì‚¬ëŒë“¤ì˜ ê²½ìš° 2ê°œë¡œ ë‚˜ëˆŒ ë•Œì™€ 3ê°œë¡œ ë‚˜ëˆŒ ë•Œ ê°€ì¥ ì ì ˆí•œ í´ëŸ¬ìŠ¤í„°ë§ì´ ë‹¤ë¥¼ ê²ƒì´ë‹¤.

      * K-meansëŠ” ë³´í†µ ì£¼ì–´ì§„ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ì— ëŒ€í•´ì„œ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë‚³ëŠ”ë‹¤ê³  ì•Œë ¤ì ¸ ìˆë‹¤.

        

  ### 1-3. í´ëŸ¬ìŠ¤í„°ë§ í‰ê°€í•˜ê¸° (Clustering Evaluation)

  * ì ì ˆí•œ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ êµ¬í•˜ê¸°

    #### 1-3-1. **Elbow method**

    * costì˜ ê°ì†Œ ì†ë„ê°€ ì¤„ì–´ë“œëŠ” ì§€ì ì—ì„œ ëŠê¸°

      ![](D:\soyeon.kim\2. ê°œì¸\images\50.PNG)

    * **ì•Œê³ ë¦¬ì¦˜**

      * ê·¸ë˜í”„ë¥¼ ë³´ë©´ ì²˜ìŒì—ëŠ” ê¸‰ê²©íˆ costê°€ ê°ì†Œí•˜ë‹¤ê°€ ì–´ëŠ ìˆœê°„ ì •ì²´ëœë‹¤.
      * ê°ì†Œì„¸ê°€ ì •ì²´ë˜ëŠ” ì§€ì ì—ì„œ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ë¥¼ ì •í•˜ì

    * **í•œê³„**

      * ê°ì†Œì„¸ê°€ ì •ì²´ë˜ëŠ” ë¶€ë¶„ì´ ì–¸ì œì¸ì§€ ê²°ì •í•˜ëŠ” ê²ƒì€ ì£¼ê´€ì ì´ë‹¤

      * ê°ì†Œë¶„ì´ ìµœì†Œì¸ ì§€ì ì´ í•­ìƒ ê°€ì¥ ì¢‹ì€ í´ëŸ¬ìŠ¤í„°ë§ì€ ì•„ë‹ˆë‹¤.

        

    ### 1-3-2. Silhouette

    ì‹¤ë£¨ì—£ ê°’ì€ í•œ í´ëŸ¬ìŠ¤í„° ì•ˆì˜ ë°ì´í„°ë“¤ì´ ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ì™€ ë¹„êµí•´ì„œ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œê°€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.

    - í´ëŸ¬ìŠ¤í„° ì•ˆì˜ ê±°ë¦¬ê°€ ì§§ì„ ìˆ˜ë¡ ì¢‹ê³ (cohesion) ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ì™€ì˜ ê±°ë¦¬ëŠ” ë©€ìˆ˜ë¡ ì¢‹ë‹¤.(separation)

    - ë°ì´í„° í¬ì¸íŠ¸ ë³„ë¡œ ê°’ì„ êµ¬í•  ìˆ˜ ìˆë‹¤.

    - ì‹¤ë£¨ì—£ì€ -1ë¶€í„° 1 ì‚¬ì´ì˜ ê°’ì„ ê°€ì§„ë‹¤.

    - ë†’ì„ ìˆ˜ë¡ ì¢‹ë‹¤.

    - ì–´ë–¤ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ì´ë“ ì§€ êµ¬í•  ìˆ˜ ìˆë‹¤.

    - **ê³„ì‚° ë°©ë²•**

      - ië²ˆì§¸ ë°ì´í„° í¬ì¸íŠ¸ì— ëŒ€í•´ì„œ
      - a(i)ëŠ” ê°™ì€ í´ëŸ¬ìŠ¤í„° ì•ˆì— ìˆëŠ” ë‹¤ë¥¸ ë°ì´í„° í¬ì¸íŠ¸ì™€ì˜ í‰ê·  ê±°ë¦¬(dissimilarity)
        - a(i)ëŠ” ië²ˆì§¸ ë°ì´í„° í¬ì¸íŠ¸ê°€ í´ëŸ¬ìŠ¤í„°ì— ì–¼ë§ˆë‚˜ ì˜ ë§ëŠ”ì§€ ì¸¡ì •í•œë‹¤.
        - ë‚®ì„ ìˆ˜ë¡ ë” ì˜ ì†í•´ìˆëŠ” ê²ƒ
      - b(i)ëŠ” iê°€ ì†í•˜ì§€ ì•Šì€ ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ì™€ì˜ í‰ê·  ê±°ë¦¬ ì¤‘ ê°€ì¥ ì‘ì€ ê±°ë¦¬ì´ë‹¤.
        - ë‚´ê°€ ì†í•˜ì§€ ì•Šì€ ê°€ì¥ ê°€ê¹Œìš´ í´ëŸ¬ìŠ¤í„°, ì¦‰ ì´ì›ƒ í´ëŸ¬ìŠ¤í„°(neighbouring cluster)ì™€ì˜ ê±°ë¦¬

    - **ì‹¤ë£¨ì—£ ì •ì˜**

      ![](D:\soyeon.kim\2. ê°œì¸\images\51.png)

      - b(i)ê°€ a(i)ë³´ë‹¤ ë§ì´ í´ìˆ˜ë¡ 1ì— ê°€ê¹Œì›Œì§€ê³ , ì¢‹ë‹¤.

      - 0ì´ë©´ ì§€ê¸ˆ í´ëŸ¬ìŠ¤í„°ë‚˜ ì´ì›ƒ í´ëŸ¬ìŠ¤í„°ë‚˜ ì–´ë”” ìˆë“  ìƒê´€ ì—†ë‹¤ëŠ” ì–˜ê¸°

        

  ### 1-4. Clustering ì‹¤ìŠµ

  ```python
  import numpy as np
  # generate two clusters: a with 100 points, b with 50:
  np.random.seed(4711)  # for repeatability of this tutorial
  a = np.random.multivariate_normal([10, 0], [[3, 1], [1, 4]], size=[100,])
  b = np.random.multivariate_normal([0, 20], [[3, 1], [1, 4]], size=[50,])
  X = np.concatenate((a, b),)
  
  from matplotlib import pyplot as plt
  %matplotlib inline
  
  plt.scatter(X[:,0], X[:,1])
  plt.show()
  ```

  ![](D:\soyeon.kim\2. ê°œì¸\images\52.PNG)
  * **Hierarchical**

    ```python
    from sklearn.cluster.hierarchical import AgglomerativeClustering
    AgglomerativeClustering??
    ```

    * **Dendrogram**

      ```python
      from scipy.cluster.hierarchy import dendrogram, linkage
      Z = linkage(X, 'ward')
      ```

      *`ward`ëŠ” í´ëŸ¬ìŠ¤í„° ë‚´ ì´ ë¶„ì‚°(total within-cluster variance)ì„ ìµœì†Œí™” ì‹œí‚¤ëŠ” linkage ë°©ë²•*

  * **Z ë“¤ì—¬ë‹¤ë³´ê¸°**

    ```python
    Z[:10].round(0)
    ```

    ![](D:\soyeon.kim\2. ê°œì¸\images\53.PNG)

    ```python
    plt.figure(figsize=(25, 10))
    plt.title('Hierarchical Clustering Dendrogram')
    plt.xlabel('sample index')
    plt.ylabel('distance')
    dendrogram(
        Z,
        leaf_rotation=90.,  # rotates the x axis labels
        leaf_font_size=8.,  # font size for the x axis labels
        color_threshold=5
    )
    plt.show()
    ```

    ![](D:\soyeon.kim\2. ê°œì¸\images\54.PNG)

  * **í´ëŸ¬ìŠ¤í„°ë³„ ìƒ‰ê¹” ë‹¤ë¥´ê²Œ í•˜ê¸°**

    `color_threshold`

    ```python
    plt.title('Hierarchical Clustering Dendrogram (truncated)')
    plt.xlabel('sample index')
    plt.ylabel('distance')
    dendrogram(
        Z,
        truncate_mode='lastp',  # show only the last p merged clusters
        p=12,  # show only the last pa merged clusters
        show_leaf_counts=False,  # otherwise numbers in brackets are counts
        leaf_rotation=90.,
        leaf_font_size=12.,
        show_contracted=True,  # to get a distribution impression in truncated branches
        color_threshold=50
    )
    plt.show()
    ```

    ![](D:\soyeon.kim\2. ê°œì¸\images\55.PNG)

  * **ì••ì¶•í•˜ê¸°**

    ```python
    plt.title('Hierarchical Clustering Dendrogram (truncated)')
    plt.xlabel('sample index or (cluster size)')
    plt.ylabel('distance')
    dendrogram(
        Z,
        truncate_mode='lastp',  # show only the last p merged clusters
        p=12,  # show only the last p merged clusters
        leaf_rotation=90.,
        leaf_font_size=12.,
        show_contracted=True,  # to get a distribution impression in truncated branches
    )
    plt.show()
    ```

    ![](D:\soyeon.kim\2. ê°œì¸\images\56.PNG)

  * **merge ë  ë•Œ distanceë¥¼ í‘œì‹œí•˜ê¸°**

    ```python
    def fancy_dendrogram(*args, **kwargs):
        max_d = kwargs.pop('max_d', None)
        if max_d and 'color_threshold' not in kwargs:
            kwargs['color_threshold'] = max_d
        annotate_above = kwargs.pop('annotate_above', 0)
    
        ddata = dendrogram(*args, **kwargs)
    
        if not kwargs.get('no_plot', False):
            plt.title('Hierarchical Clustering Dendrogram (truncated)')
            plt.xlabel('sample index or (cluster size)')
            plt.ylabel('distance')
            for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):
                x = 0.5 * sum(i[1:3])
                y = d[1]
                if y > annotate_above:
                    plt.plot(x, y, 'o', c=c)
                    plt.annotate("%.3g" % y, (x, y), xytext=(0, -5),
                                 textcoords='offset points',
                                 va='top', ha='center')
            if max_d:
                plt.axhline(y=max_d, c='k')
        return ddata
    
    fancy_dendrogram(
        Z,
        truncate_mode='lastp',
        p=12,
        leaf_rotation=90.,
        leaf_font_size=12.,
        show_contracted=True,
        annotate_above=10,  # useful in small plots so annotations don't overlap
    )
    plt.show()
    ```

  * **ìë¥´ëŠ” ê¸°ì¤€ì„ í‘œì‹œí•˜ê¸°**

    ```python
    max_d = 20
    
    fancy_dendrogram(
        Z,
        truncate_mode='lastp',
        p=12,
        leaf_rotation=90.,
        leaf_font_size=12.,
        show_contracted=True,
        annotate_above=3,
        max_d=max_d,  # plot a horizontal cut-off line
    )
    plt.show()
    ```

  * **label ë½‘ì•„ë‚´ê¸° - max distanceë¡œ**

    ```python
    from scipy.cluster.hierarchy import fcluster
    max_d = 5
    clusters = fcluster(Z, max_d, criterion='distance')
    clusters
    ```

  * **label ë½‘ì•„ë‚´ê¸° - í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ë¡œ**

    ```python
    k=2
    fcluster(Z, k, criterion='maxclust')
    ```

  * **Evaluation**

    ```python
    import pandas as pd
    
    from sklearn.datasets import load_iris
    
    iris = load_iris()
    
    X = pd.DataFrame(iris.data, columns=iris.feature_names)
    X.head()
    ```

    ![](D:\soyeon.kim\2. ê°œì¸\images\57.PNG)

    ```python
    from sklearn.cluster import KMeans
    
    km = KMeans(n_clusters=8)
    km.fit(X)
    ```

    ![](D:\soyeon.kim\2. ê°œì¸\images\58.PNG)

    ```python
    from tqdm import tqdm_notebook
    
    scores = []
    iterations = range(1, 30)
    for i in tqdm_notebook(iterations):
        km = KMeans(n_clusters=i)
        km.fit(X)
        s = km.score(X)
        scores.append(s)
        
    from matplotlib import pyplot as plt
    %matplotlib inline
    
    plt.figure(figsize=(17,5))
    plt.plot(iterations, scores, 'b*-')
    ```

    ![](D:\soyeon.kim\2. ê°œì¸\images\59.PNG)

    ```python
    from sklearn.metrics import silhouette_score, silhouette_samples
    
    K = range(2,30)
    
    scores = []
    for k in tqdm_notebook(K):
        kmeans = KMeans(n_clusters=k)
        kmeans.fit(X)
    
        labels = kmeans.labels_
    #     centroids = kmeans.cluster_centers_
        score = silhouette_score(X, labels, metric='euclidean')
        scores.append(score)
    
    plt.plot(K, scores)
    plt.ylabel("Silouette")
    plt.xlabel("k")
    plt.title("Silouette for K-means cell's behaviour")
    ```

    ![](D:\soyeon.kim\2. ê°œì¸\images\60.PNG)



## 2. EDA

* dd

  ### 2-1. ë¬¸ì œ ì •ì˜í•˜ê¸°

  * Mall Customer Segmentation Data ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ê³ ê°ì„ ë¶„ë¥˜í•œë‹¤.

  ### 2-2. ë°ì´í„° í™•ì¸í•˜ê¸°

  ```python
  import numpy as np # linear algebra
  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
  import matplotlib.pyplot as plt 
  plt.style.use('fivethirtyeight')
  import seaborn as sns 
  import plotly as py
  import plotly.graph_objs as go
  from sklearn.cluster import KMeans
  import warnings
  import os
  warnings.filterwarnings("ignore")
  py.offline.init_notebook_mode(connected = True)
  
  df = pd.read_csv('./data/mall_customer_seg_data/Mall_Customers.csv')
  df.head()
  ```

  ![](D:\soyeon.kim\2. ê°œì¸\images\61.PNG)
  * **Histograms**

    ```python
    plt.figure(1 , figsize = (15 , 6))
    n = 0 
    for x in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:
        n += 1
        plt.subplot(1 , 3 , n)
        plt.subplots_adjust(hspace =0.5 , wspace = 0.5)
        sns.distplot(df[x] , bins = 20)
        plt.title('Distplot of {}'.format(x))
    plt.show()
    ```

    ![](D:\soyeon.kim\2. ê°œì¸\images\62.PNG)



## 3. Preprocessing

*No need to preprocess*



## 4. Customer Segmentation

* Clustering using K-means

  ### 4.1 Segmentation using Age and Spending Score

  ```python
  '''Age and spending Score'''
  X1 = df[['Age' , 'Spending Score (1-100)']].iloc[: , :].values
  inertia = []
  for n in range(1 , 11):
      algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, 
                          tol=0.0001,  random_state= 111  , algorithm='elkan') )
      algorithm.fit(X1)
      inertia.append(algorithm.inertia_)
  ```

  *Selecting N Clusters based in Inertia (ê´€ì„±, Squared Distance between Centroids and data points, should be less)*

  ```python
  plt.figure(1 , figsize = (15 ,6))
  plt.plot(np.arange(1 , 11) , inertia , 'o')
  plt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)
  plt.xlabel('Number of Clusters') , plt.ylabel('Inertia')
  plt.show()
  ```

  ![](D:\soyeon.kim\2. ê°œì¸\images\63.PNG)

  ```python
  algorithm = (KMeans(n_clusters = 4 ,init='k-means++', n_init = 10 ,max_iter=300, 
                          tol=0.0001,  random_state= 111  , algorithm='elkan') )
  algorithm.fit(X1)
  labels1 = algorithm.labels_
  centroids1 = algorithm.cluster_centers_
  ```

  ```python
  h = 0.02
  x_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1
  y_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1
  xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
  Z = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) 
  ```

  ```python
  plt.figure(1 , figsize = (15 , 7) )
  plt.clf()
  Z = Z.reshape(xx.shape)
  plt.imshow(Z , interpolation='nearest', 
             extent=(xx.min(), xx.max(), yy.min(), yy.max()),
             cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')
  
  plt.scatter( x = 'Age' ,y = 'Spending Score (1-100)' , data = df , c = labels1 , 
              s = 200 )
  plt.scatter(x = centroids1[: , 0] , y =  centroids1[: , 1] , s = 300 , c = 'red' , alpha = 0.5)
  plt.ylabel('Spending Score (1-100)') , plt.xlabel('Age')
  plt.show()
  ```

  ![](D:\soyeon.kim\2. ê°œì¸\images\64.PNG)

  

  ### 4-2. Segmentation using Annual Income and Spending Score

  ```python
  '''Annual Income and spending Score'''
  X2 = df[['Annual Income (k$)' , 'Spending Score (1-100)']].iloc[: , :].values
  inertia = []
  for n in range(1 , 11):
      algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, 
                          tol=0.0001,  random_state= 111  , algorithm='elkan') )
      algorithm.fit(X2)
      inertia.append(algorithm.inertia_)
  ```

  ```python
  plt.figure(1 , figsize = (15 ,6))
  plt.plot(np.arange(1 , 11) , inertia , 'o')
  plt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)
  plt.xlabel('Number of Clusters') , plt.ylabel('Inertia')
  plt.show()
  ```

  ![](D:\soyeon.kim\2. ê°œì¸\images\65.PNG)

  ```python
  algorithm = (KMeans(n_clusters = 5 ,init='k-means++', n_init = 10 ,max_iter=300, 
                          tol=0.0001,  random_state= 111  , algorithm='elkan') )
  algorithm.fit(X2)
  labels2 = algorithm.labels_
  centroids2 = algorithm.cluster_centers_
  ```

  ```python
  h = 0.02
  x_min, x_max = X2[:, 0].min() - 1, X2[:, 0].max() + 1
  y_min, y_max = X2[:, 1].min() - 1, X2[:, 1].max() + 1
  xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
  Z2 = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) 
  ```

  ```python
  plt.figure(1 , figsize = (15 , 7) )
  plt.clf()
  Z2 = Z2.reshape(xx.shape)
  plt.imshow(Z2 , interpolation='nearest', 
             extent=(xx.min(), xx.max(), yy.min(), yy.max()),
             cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')
  
  plt.scatter( x = 'Annual Income (k$)' ,y = 'Spending Score (1-100)' , data = df , c = labels2 , 
              s = 200 )
  plt.scatter(x = centroids2[: , 0] , y =  centroids2[: , 1] , s = 300 , c = 'red' , alpha = 0.5)
  plt.ylabel('Spending Score (1-100)') , plt.xlabel('Annual Income (k$)')
  plt.show()
  ```

  ![](D:\soyeon.kim\2. ê°œì¸\images\66.PNG)

  

  ### 4-3. Segmentation using Age , Annual Income and Spending Score

  ```python
  X3 = df[['Age' , 'Annual Income (k$)' ,'Spending Score (1-100)']].iloc[: , :].values
  inertia = []
  for n in range(1 , 11):
      algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, 
                          tol=0.0001,  random_state= 111  , algorithm='elkan') )
      algorithm.fit(X3)
      inertia.append(algorithm.inertia_)
  ```

  ```python
  plt.figure(1 , figsize = (15 ,6))
  plt.plot(np.arange(1 , 11) , inertia , 'o')
  plt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)
  plt.xlabel('Number of Clusters') , plt.ylabel('Inertia')
  plt.show()
  ```

  ![](D:\soyeon.kim\2. ê°œì¸\images\67.png)

  ```python
  algorithm = (KMeans(n_clusters = 6 ,init='k-means++', n_init = 10 ,max_iter=300, 
                          tol=0.0001,  random_state= 111  , algorithm='elkan') )
  algorithm.fit(X3)
  labels3 = algorithm.labels_
  centroids3 = algorithm.cluster_centers_
  ```

  ```python
  df['label3'] =  labels3
  trace1 = go.Scatter3d(
      x= df['Age'],
      y= df['Spending Score (1-100)'],
      z= df['Annual Income (k$)'],
      mode='markers',
       marker=dict(
          color = df['label3'], 
          size= 20,
          line=dict(
              color= df['label3'],
              width= 12
          ),
          opacity=0.8
       )
  )
  data = [trace1]
  layout = go.Layout(
  #     margin=dict(
  #         l=0,
  #         r=0,
  #         b=0,
  #         t=0
  #     )
      title= 'Clusters',
      scene = dict(
              xaxis = dict(title  = 'Age'),
              yaxis = dict(title  = 'Spending Score'),
              zaxis = dict(title  = 'Annual Income')
          )
  )
  fig = go.Figure(data=data, layout=layout)
  py.offline.iplot(fig)
  ```

  ![](D:\soyeon.kim\2. ê°œì¸\images\68.PNG)