----

title: "Data Science - Classification"

date: 2019-08-28 15:18

categories: development python data-science

------



íƒ€ì´íƒ€ë‹‰ ì¹¨ëª° ì‚¬ê³ ì˜ ì‹¤ì œ ë°ì´í„°ë¥¼ í†µí•´, ìƒì¡´ìœ ë¬´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œë¥¼ Predictive Analytics ê¸°ë²•ìœ¼ë¡œ í’€ì–´ë³´ê¸°



## 1. Classification Overview

* **ì´ì§„ë¶„ë¥˜ (Binary Classification)**

  * ì´ë©”ì¼ : ìŠ¤íŒ¸ / ìŠ¤íŒ¸ ì•„ë‹˜
  * ì˜¨ë¼ì¸ ê±°ë˜ : ë„ìš© / ë„ìš© ì•„ë‹˜
  * ì¢…ì–‘ : ì‹¬ê°í•¨(Malignant) / ì‹¬ê°í•˜ì§€ ì•ŠìŒ(Benign)
  * íŠ¹ì§•: ì¢…ì†ë³€ìˆ˜(y)ê°€ 0 ë˜ëŠ” 1ì˜ ê°’ì„ ê°€ì§„ë‹¤.

* **ìœ ë°©ì•” ë°ì´í„° ì˜ˆì œ**

  * ë°ì´í„° ê°€ì ¸ì˜¤ê¸°

    ```python
    from sklearn.datasets import load_breast_cancer
    
    breast_cancer = load_breast_cancer()
    
    print(breast_cancer.DESCR)
    
    import pandas as pd
    
    data = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)
    data['malignant'] = 1 - breast_cancer.target # 0 for malignant and 1 for benign in the original dataset
    ```

  * ê·¸ë˜í”„ ê·¸ë¦¬ê¸°

    ```python
    %matplotlib inline
    from matplotlib import pyplot as plt
    
    data.plot(x='mean radius', y='malignant', kind='scatter', ylim=[-1,2])
    ```

    ![](D:\soyeon.kim\2. ê°œì¸\images\12.PNG)

  * ì„ í˜• íšŒê·€(Linear Regression)ìœ¼ë¡œ ë¶„ë¥˜í•˜ë©´ ì–´ë–¨ê¹Œ?

    ```python
    from sklearn.linear_model import LinearRegression
    
    lr = LinearRegression()
    lr.fit(data[['mean radius']], data['malignant'])
    
    data.plot(x='mean radius', y='malignant', kind='scatter', ylim=[-1,2])
    plt.plot(data['mean radius'], lr.predict(data[['mean radius']]), 'skyblue');
    ```

    ![](D:\soyeon.kim\2. ê°œì¸\images\13.PNG)
    * ì„ í˜•íšŒê·€ë¡œ ëª¨ë¸ì„ ë§Œë“  ë‹¤ìŒì— â„ğœƒ(ğ‘¥)=0.5hÎ¸(x)=0.5ë³´ë‹¤ ì‘ìœ¼ëƒ í¬ëƒë¡œ yê°’ì„ ì˜ˆì¸¡í•œë‹¤.
    * ëª» í•  ê±´ ì—†ì§€ë§Œ
    * yëŠ” 0 ë˜ëŠ” 1ì´ë‹¤.
    * â„ğœƒ(ğ‘¥)hÎ¸(x)ì´ 0ë³´ë‹¤ ì‘ê±°ë‚˜ 1ë³´ë‹¤ í° ê²ƒì€ ì´ìƒí•˜ë‹¤.
    * ê·¸ë˜ì„œ 1â‰¥â„ğœƒ(ğ‘¥)â‰¥01â‰¥hÎ¸(x)â‰¥0ì¸ ëª¨ë¸ì´ í•„ìš”í•˜ë‹¤.
    * ë¡œì§€ìŠ¤í‹± íšŒê·€ (Logistic Regreesion) ëŠ” 1â‰¥â„ğœƒ(ğ‘¥)â‰¥01â‰¥hÎ¸(x)â‰¥0 ë¡œ ë§Œë“¤ì–´ì¤€ë‹¤.

  * ê°€ì„¤ (Hypothesis)

    * Goal: 1â‰¥â„ğœƒ(ğ‘¥)â‰¥0

    * Solution: ğ‘”(ğœƒğ‘‡ğ‘¥)

    * **Sigmoid function (Logistic function)**: ğ‘”(ğ‘§)=11+ğ‘’âˆ’ğ‘§

      ```python
      import numpy as np
      z = np.linspace(-4, 4)
      plt.plot(z, [1/(np.e**(-i) + 1) for i in z])
      ```

      ![](D:\soyeon.kim\2. ê°œì¸\images\14.PNG)

      -> Sigmoid functionì˜ íŠ¹ì§•: í•­ìƒ 0ë³´ë‹¤ í¬ê³  1ë³´ë‹¤ ì‘ì€ ê°’ì„ ë¦¬í„´í•¨

* **Dicistion boundary**

  * ë¡œì§€ìŠ¤í‹± íšŒê·€ (Logistic regression)ëŠ” â„ğœƒ(ğ‘¥)hÎ¸(x) >= 0.5ì¼ ë•Œ y=1ë¡œ ì˜ˆì¸¡í•œë‹¤.

    ì´ëŠ” ğ‘§=ğœƒğ‘‡ğ‘¥z=Î¸Tx > 0ì¼ ë•Œì™€ ê°™ë‹¤.

    ì¦‰, ğœƒğ‘‡ğ‘¥Î¸Tx = 0 ì¼ ë•Œë¥¼ ê¸°ì¤€ìœ¼ë¡œ class ê°€ ë‚˜ë‰˜ê²Œ ëœë‹¤.

* **Cost function**

  * â„ğœƒ(ğ‘¥)=11+ğ‘’âˆ’ğœƒğ‘‡ğ‘¥

  * ë§¤ê°œë³€ìˆ˜ (parameters) ğœƒÎ¸ë¥¼ ì–´ë–»ê²Œ êµ¬í• ê¹Œ?

    * ğ¶ğ‘œğ‘ ğ‘¡(â„ğœƒ(ğ‘¥(ğ‘–)),ğ‘¦(ğ‘–)))={âˆ’log(â„ğœƒ(ğ‘¥(ğ‘–))),âˆ’log(1âˆ’â„ğœƒ(ğ‘¥(ğ‘–))),if ğ‘¦=1if ğ‘¦=0Cost(hÎ¸(x(i)),y(i)))={âˆ’logâ¡(hÎ¸(x(i))),if y=1âˆ’logâ¡(1âˆ’hÎ¸(x(i))),if y=0

    * y = 1ì¼ ë•Œ, â„ğœƒ(ğ‘¥)hÎ¸(x) = 1ì´ë©´ costê°€ 0ì´ê³ , â„ğœƒ(ğ‘¥)hÎ¸(x) = 1ì—ì„œ ë©€ì–´ì§ˆ ìˆ˜ë¡ ë¬´í•œëŒ€ì— ê°€ê¹Œì›Œì§„ë‹¤.

      ```python
      h_range = np.linspace(0.001, 0.999, num=1000)
      plt.plot(h_range, -np.log(h_range))
      ```

      ![](D:\soyeon.kim\2. ê°œì¸\images\15.PNG)

    * y = 0ì¼ ë•Œ, â„ğœƒ(ğ‘¥)hÎ¸(x) = 0ì´ë©´ costê°€ 0ì´ê³ , â„ğœƒ(ğ‘¥)hÎ¸(x) = 0ì—ì„œ ë©€ì–´ì§ˆ ìˆ˜ë¡ ë¬´í•œëŒ€ì— ê°€ê¹Œì›Œì§„ë‹¤.

      ```python
      plt.plot(h_range, -np.log(1 - h_range))
      ```

      ![](D:\soyeon.kim\2. ê°œì¸\images\16.PNG)

    * ì˜ˆì¸¡ê°’ì´ ì •ë‹µê³¼ ë§ìœ¼ë©´ costê°€ 0, ë§ì´ í‹€ë¦´ ìˆ˜ë¡ costë¥¼ ë§ì´ ì¤€ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

  * **Simplified cost function**

    * ğ½(ğœƒ)=1ğ‘›âˆ‘ğ‘–=1ğ‘›ğ¶ğ‘œğ‘ ğ‘¡(â„ğœƒ(ğ‘¥(ğ‘–),ğ‘¦(ğ‘–)))
    * ğ¶ğ‘œğ‘ ğ‘¡(â„ğœƒ(ğ‘¥(ğ‘–)),ğ‘¦(ğ‘–)))={âˆ’log(â„ğœƒ(ğ‘¥(ğ‘–))),âˆ’log(1âˆ’â„ğœƒ(ğ‘¥(ğ‘–))),if ğ‘¦=1if ğ‘¦=0
    * ğ¶ğ‘œğ‘ ğ‘¡(â„ğœƒ(ğ‘¥(ğ‘–)),ğ‘¦(ğ‘–)))=âˆ’ğ‘¦log(â„ğœƒ(ğ‘¥(ğ‘–)))âˆ’(1âˆ’ğ‘¦)ğ‘™ğ‘œğ‘”(1âˆ’â„ğœƒ(ğ‘¥(ğ‘–)))





## 1-1. ë‚˜ë¬´ ê¸°ë°˜ ëª¨ë¸ (Tree-Based Models)

* ì¼ë°˜ì ìœ¼ë¡œ ì„±ëŠ¥ì´ ë” ì¢‹ì€ê±´ ì•„ë‹ˆë‚˜, ì•™ìƒë¸”ê³¼ ê°™ì´ ì‚¬ìš©í–ˆì„ ë•Œ ì„±ëŠ¥ì´ ë†’ì•„ì§

* íšŒê·€ì™€ ë¶„ë¥˜ ë¬¸ì œì— ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥

* ì˜ˆì¸¡ë³€ìˆ˜ ê³µê°„(predictor space)ì„ ì—¬ëŸ¬ ê°œì˜ êµ¬ì—­ìœ¼ë¡œ êµ¬íš(stratifying or segmenting)í•œë‹¤.

* ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ì„œëŠ” ê·¸ ë°ì´í„°ê°€ ì†í•œ êµ¬ì—­ì— ìˆëŠ” í•™ìŠµ ë°ì´í„°ì…‹ì˜ í‰ê· ì´ë‚˜ ìµœë¹ˆê°’ì„ ì´ìš©í•œë‹¤.

* êµ¬ì—­ì„ ë‚˜ëˆ„ëŠ” ê·œì¹™ì„ ë‚˜ë¬´(tree)ë¡œ í‘œí˜„í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ì´ëŸ° ì ‘ê·¼ì„ ì˜ì‚¬ ê²°ì • ë‚˜ë¬´(decision tree)ë¡œ ë¶€ë¥¸ë‹¤.

* ì˜ì‚¬ ê²°ì • ë‚˜ë¬´(decision tree)ëŠ” ì—¬ëŸ¬ ê°€ì§€ ê·œì¹™ì„ ìˆœì°¨ì ìœ¼ë¡œ ì ìš©í•˜ë©´ì„œ ë…ë¦½ ë³€ìˆ˜ ê³µê°„ì„ ë¶„í• í•˜ëŠ” ë¶„ë¥˜ ëª¨í˜•ì´ë‹¤. ë¶„ë¥˜(classification)ì™€ íšŒê·€ ë¶„ì„(regression)ì— ëª¨ë‘ ì‚¬ìš©ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— CART(Classification And Regression Tree)ë¼ê³ ë„ í•œë‹¤.

* **ì˜ì‚¬ ê²°ì • ë‚˜ë¬´ (Decistion Trees)ì˜ íŠ¹ì§•**

  * ê°„ë‹¨í•˜ê³  í•´ì„í•˜ê¸° ì‰½ë‹¤.
  * ì •í™•ë„ëŠ” ë‹¤ì†Œ ë‚®ë‹¤.
  * ë°°ê¹…(bagging), ëœë¤ í¬ë ˆìŠ¤íŠ¸(random forests), ë¶€ìŠ¤íŒ…(boosting) ë“± ì•™ìƒë¸” ê¸°ë²•ê³¼ í•¨ê»˜ ìì£¼ ì‚¬ìš©ëœë‹¤.
  * ìˆ˜ë§ì€ ì˜ì‚¬ ê²°ì • ë‚˜ë¬´ë¥¼ ê²°í•©í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ë³´ê²Œ ë  ê²ƒì´ë‹¤.
  * ë‹¨, ì˜ì‚¬ ê²°ì • ë‚˜ë¬´ì˜ ê°¯ìˆ˜ê°€ ë§ìœ¼ë©´ ëª¨ë¸ì˜ í•´ì„ì€ ë” ì–´ë ¤ì›Œì§„ë‹¤.

  ### 1-2-1. íšŒê·€ íŠ¸ë¦¬ (Regression Tree)

  * ì˜ˆì‹œ : íšŒê·€ íŠ¸ë¦¬ë¥¼ ì´ìš©í•´ì„œ ì•¼êµ¬ ì„ ìˆ˜ì˜ ì—°ë´‰ì„ ì˜ˆì¸¡í•˜ê¸°

    * ì•¼êµ¬ ì„ ìˆ˜ì˜ ê²½ë ¥ê³¼ ì‘ë…„ì— ëª‡ ë²ˆ ê³µì„ ì³¤ëŠ”ì§€ë¥¼ ì´ìš©í•´ì„œ íƒ€ìì˜ ì—°ë´‰ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸
    * Hitters : Baseball Data : https://www.kaggle.com/floser/hitters
    * ë³€ìˆ˜ ì„¤ëª… : https://rdrr.io/cran/ISLR/man/Hitters.html

    ![](D:\soyeon.kim\2. ê°œì¸\images\17.PNG)
    * í•´ì„: ê²½ë ¥ì´ 4.5ë…„ì´ ì•ˆë˜ëŠ” ì•¼êµ¬ì„ ìˆ˜ëŠ” ì—°ë´‰ì„ 5.11ì„ ë°›ê³ , ê²½ë ¥ì´ 4.5ë…„ë³´ë‹¤ ë§ì€ ì•¼êµ¬ ì„ ìˆ˜ëŠ” ì‘ë…„ íƒ€ìˆ˜ê°€ 117.5ë³´ë‹¤ ì‘ìœ¼ë©´ 6.00, í¬ë©´ 6.74ë¥¼ ë°›ëŠ”ë‹¤.
      - ì •í™•íˆëŠ” ì—°ë´‰ì˜ logë¥¼ ì·¨í•œ ê°’ì„ ì˜ˆì¸¡
      - ê·¸ë˜ì„œ 1,000Ã—ğ‘’6.740=1,000Ã—e6.740=845,346
      - ì ë…¸ë“œì— ìˆëŠ” ìˆ«ìëŠ” í•™ìŠµ ë°ì´í„° ì¤‘ì— í•´ë‹¹ êµ¬ì—­ìœ¼ë¡œ ë–¨ì–´ì§„ ì•¼êµ¬ ì„ ìˆ˜ë“¤ ì—°ë´‰ì˜ í‰ê· ì´ë‹¤.

    ![](D:\soyeon.kim\2. ê°œì¸\images\18.PNG)

    * ì„¸ êµ¬ì—­ì€ ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„ ê°€ëŠ¥í•˜ë‹¤.
      - R1 ={X | Years<4.5}
      - R2 ={X | Years>=4.5, Hits<117.5}
      - R3 ={X | Years>=4.5, Hits>=117.5}
    *   í•´ì„
      * ê²½ë ¥ì€ ì—°ë´‰ì„ ê²°ì •í•˜ëŠ” ë° ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†Œì´ë‹¤.
      * ê²½ë ¥ì´ ì ìœ¼ë©´ ì—°ë´‰ì´ í‰ê· ì ìœ¼ë¡œ ë‚®ë‹¤.
      * ì•¼êµ¬ì„ ìˆ˜ì˜ ê²½ë ¥ì´ ì ìœ¼ë©´, ì‘ë…„ì— ëª‡ë²ˆ ê³µì„ ì³¤ëŠ”ì§€ëŠ” ì¤‘ìš”í•˜ì§€ ì•Šë‹¤.
      * ê·¸ëŸ¬ë‚˜ ì•¼êµ¬ì„ ìˆ˜ì˜ ê²½ë ¥ì´ 5ë…„ ì´ìƒì´ë©´, ì‘ë…„ì— ê¸°ë¡í•œ íƒ€ìˆ˜ê°€ ì—°ë´‰ì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤.
      * íƒ€ìˆ˜ê°€ ë†’ì€ íƒ€ìëŠ” ë‹¤ë¥¸ íƒ€ìë“¤ë³´ë‹¤ í‰ê· ì ìœ¼ë¡œ ì—°ë´‰ì„ ë§ì´ ë°›ëŠ”ë‹¤.

  * **íšŒê·€ íŠ¸ë¦¬ (regression tree) ë§Œë“¤ê¸°**

    ![](D:\soyeon.kim\2. ê°œì¸\images\19.PNG)

  * **Splitting**

    * ì˜ˆì¸¡ ë³€ìˆ˜ ê³µê°„(predictor space)ì„ Jê°œì˜ ì„œë¡œ ë‹¤ë¥¸ êµ¬ì—­ìœ¼ë¡œ ë‚˜ëˆˆë‹¤.
    * ì˜ˆì¸¡ ë³€ìˆ˜ ê³µê°„(predictor space)ì´ë€ ì˜ˆì¸¡ ë³€ìˆ˜ X1, X2, â€¦, Xpê°€ ì·¨í•  ìˆ˜ ìˆëŠ” ê°’ë“¤ì˜ ì¡°í•©ì„ ë§í•œë‹¤.
    * ìš°ë¦¬ì˜ ëª©ì ì€ RSSë¥¼ ìµœì†Œí™”í•˜ëŠ” R1, â€¦, RJë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤.
    * âˆ‘ğ‘—=1ğ½âˆ‘ğ‘–âˆˆğ‘…ğ‘—(ğ‘¦ğ‘–âˆ’ğ‘¦Ì‚ ğ‘…ğ‘—)2âˆ‘j=1Jâˆ‘iâˆˆRj(yiâˆ’y^Rj)2
    * ìš°ë¦¬ëŠ” ì˜ˆì¸¡ ë³€ìˆ˜ ê³µê°„ì„ ê³ ì°¨ì›ì˜ ì§ì‚¬ê°í˜•ë“¤(high dimensional rectanngles), ì¦‰ ë°•ìŠ¤ë“¤(boxes)ë¡œ ë‚˜ëˆˆë‹¤.

  * **Pruning**

    * Spliting ë°©ì‹ì€ training dataì—ëŠ” ì˜ ì í•©í•  ìˆ˜ ìˆìœ¼ë‚˜, overfitì˜ ìœ„í—˜ì´ ìˆë‹¤. ì§€ë‚˜ì¹˜ê²Œ ì„¸ë¶„í™”ëœ treeëŠ” train setì—ì„œë§Œì˜ íŠ¹ì§•ë§ˆì € ë°˜ì˜ í•´ë²„ë¦´ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ì´ëŠ” ê²°ê³¼ì ìœ¼ë¡œ test setì—ì„œëŠ” ì¢‹ì§€ ëª»í•œ ì„±ëŠ¥ì„ ëƒ„ì„ ë‚˜íƒ€ë‚¸ë‹¤. ë”°ë¼ì„œ Pruning ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ë©´ splitì„ ì œí•œí•˜ì—¬ ì•½ê°„ì˜ biasê°€ ìƒê¸°ë”ë¼ë„ varianceê°€ ë†’ì§€ ì•Šì€, â€˜ì§€ë‚˜ì¹˜ê²Œ ì„¸ë¶„í™”ë˜ì§€ ì•Šì€ treeâ€™ë¥¼ ë§Œë“¤ìˆ˜ ìˆë‹¤.
    * ì´ ë•Œ ì‚¬ìš©í•˜ëŠ” Pruning ì•Œê³ ë¦¬ì¦˜ì´ Cost complexity pruning ì´ë‹¤. ì´ëŠ” ìƒˆë¡œìš´ splitì„ í•¨ìœ¼ë¡œì¨ ìƒê¸°ëŠ” RSSì˜ â€˜ì´ë“â€™ì´ Î±|T| ë¼ëŠ” penaltyë³´ë‹¤ ë” í¬ì§€ ëª»í•˜ë‹¤ë©´ splitì„ ì•ˆí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.

  * **íšŒê·€ íŠ¸ë¦¬ ì•Œê³ ë¦¬ì¦˜ ì •ë¦¬**

    * Xjë¥¼ ì„ íƒí•˜ê³  ë‚˜ëˆ„ëŠ” ê¸°ì¤€(cutpoint) së¥¼ ì„ íƒí•´ì„œ ë‘ ê°œì˜ êµ¬ì—­ìœ¼ë¡œ ë‚˜ëˆˆë‹¤.
    * ê·¸ êµ¬ì—­ì—ì„œ ìœ„ì—ì„œ ì–¸ê¸‰í•œëŒ€ë¡œ í‰ê· ê°’ì„ ì´ìš©í•´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•œë‹¤.
    * ëª¨ë“  ì˜ˆì¸¡ ë³€ìˆ˜ì™€ ëª¨ë“  ê°€ëŠ¥í•œ ë‚˜ëˆ„ëŠ” ê¸°ì¤€(cutpoint)ì— ëŒ€í•´ì„œ RSSë¥¼ êµ¬í•œë‹¤.
    * ì´ ë•Œ ê¸°ì¡´ë³´ë‹¤ ê°€ì¥ RSSë¥¼ í¬ê²Œ ë‚®ì¶”ëŠ” Xjì™€ së¥¼ ì„ íƒí•œë‹¤.
    * ê·¸ ë‹¤ìŒì—ëŠ”, ë‚˜ëˆ ì§„ êµ¬ì—­ ì¤‘ í•˜ë‚˜ì—ì„œ ê°™ì€ ì¼ì„ ë°˜ë³µí•œë‹¤.
    * ì´ ì¼ì„ ë¯¸ë¦¬ ì •í•œ ë©ˆì¶”ëŠ” ê¸°ì¤€(stoppinng criterion)ì´ ë„ë‹¬í•  ë•Œê¹Œì§€ ë°˜ë³µí•œë‹¤.
    * ì˜ˆì»¨ëŒ€, ëª¨ë“  êµ¬ì—­(region)ì— í•™ìŠµ ë°ì´í„°ê°€ 5ê°œ ë¯¸ë§Œìœ¼ë¡œ ë‚¨ì•˜ì„ ë•Œ ê·¸ë§Œë‘˜ ìˆ˜ ìˆë‹¤.

  

  ### 1-2-2. ë¶„ë¥˜ íŠ¸ë¦¬ (Classification Tree)

    * íšŒê·€ íŠ¸ë¦¬ì™€ ê±°ì˜ ìœ ì‚¬í•˜ë‹¤.

  * ì–‘ì ì¸(quantitative) ë°˜ì‘ë³€ìˆ˜ê°€ ì•„ë‹Œ ì§ˆì ì¸(qualitative) ë°˜ì‘ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.

  * ë¶„ë¥˜ íŠ¸ë¦¬ì—ì„œ, ì˜ˆì¸¡ì€ ê·¸ êµ¬ì—­ì—ì„œ í‰ê· ì´ ì•„ë‹Œ **ê°€ì¥ ìì£¼ ë“±ì¥í•˜ëŠ” í´ë˜ìŠ¤**ë¡œ ê²°ì •ëœë‹¤.

    * **ë¶„ë¥˜ íŠ¸ë¦¬ ë§Œë“¤ê¸°**

        * Heart Disease UCI : https://www.kaggle.com/ronitf/heart-disease-uci
      * ë³€ìˆ˜ ì„¤ëª… : https://archive.ics.uci.edu/ml/datasets/Heart+Disease![](D:\soyeon.kim\2. ê°œì¸\images\20.PNG)
      * íšŒê·€ íŠ¸ë¦¬ì™€ ê±°ì˜ ìœ ì‚¬í•˜ë‹¤.
      * ë‹¤ë§Œ RSSëŠ” ë”ì´ìƒ ì“¸ ìˆ˜ ì—†ë‹¤.

  * **ë¶„ë¥˜ íŠ¸ë¦¬ì—ì„œ RSS ëŒ€ì‹  ì‚¬ìš©í•˜ëŠ” ê²ƒë“¤**

    * Classification error rate

      * ì˜ˆì¸¡ì´ í‹€ë¦¬ëŠ” ë¹„ìœ¨
      * ê°€ì¥ ìì£¼ ë“±ì¥í•˜ëŠ” í´ë˜ìŠ¤ë¡œ ì˜ˆì¸¡í•˜ê¸° ë•Œë¬¸ì—, ë‹¨ìˆœíˆ ê·¸ êµ¬ì—­ì—ì„œ ê°€ì¥ ìì£¼ ë“±ì¥í•˜ëŠ” í´ë˜ìŠ¤ê°€ ì•„ë‹Œ í•™ìŠµ ë°ì´í„°ì˜ ë¹„ìœ¨ì´ ëœë‹¤.

    * Gini index

      * ğº=âˆ‘ğ‘˜=1ğ¾ğ‘Ì‚ ğ‘šğ‘˜(1âˆ’ğ‘Ì‚ ğ‘šğ‘˜)

    * Entropy

      * 0â‰¤âˆ’ğ‘Ì‚ ğ‘šğ‘˜logğ‘Ì‚ ğ‘šğ‘˜

        

  ### 1-2-3. íŠ¸ë¦¬ ëª¨ë¸(Trees Models)ê³¼ ì„ í˜• ëª¨ë¸(Linear Models)ì˜ ë¹„êµ

  - ì–´ë–¤ ëª¨ë¸ì´ ë” ì¢‹ì„ì§€ëŠ” ì£¼ì–´ì§„ ë¬¸ì œì— ë‹¬ë ¤ìˆë‹¤.

  - íŠ¹ì§• ë³€ìˆ˜(features)ì™€ ë°˜ì‘ ë³€ìˆ˜(responnse)ì˜ ê´€ê³„ê°€ ì„ í˜•ì ì´ë©´, ì„ í˜• ëª¨ë¸ì´ ì˜ ë™ì‘í•œë‹¤.

  - íŠ¹ì§• ë³€ìˆ˜(features)ì™€ ë°˜ì‘ ë³€ìˆ˜(responnse)ì˜ ê´€ê³„ê°€ ì‹¬í•˜ê²Œ ë¹„ì„ í˜•ì ì´ë©´, íŠ¸ë¦¬ ëª¨ë¸ì´ ë” ì˜í•  ìˆ˜ ìˆë‹¤.

    ![](D:\soyeon.kim\2. ê°œì¸\images\21.PNG)

  

  ### 1-2-4. ì˜ì‚¬ ê²°ì • ë‚˜ë¬´ (Decision Tree) ìš”ì•½

  * *ì¥ì *
    * ì„¤ëª…í•˜ê¸° ì‰½ë‹¤. ì‹¬ì§€ì–´ ì„ í˜• íšŒê·€ë³´ë‹¤ë„ ì„¤ëª…í•˜ê¸° ì‰½ë‹¤.
    * ì‚¬ëŒì´ ì˜ì‚¬ ê²°ì •í•˜ëŠ” ë°©ì‹ì„ ì˜ í‘œí˜„í•œë‹¤.
    * ë‚˜ë¬´ë¥¼ ê·¸ë ¤ë³¼ ìˆ˜ ìˆë‹¤.
    * ì§ˆì  ë³€ìˆ˜(ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜)ë¥¼ ë”ë¯¸í™”í•˜ì§€ ì•Šê³ ë„ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ë£° ìˆ˜ ìˆë‹¤.
  * *ë‹¨ì *
    * ì˜ì‚¬ ê²°ì • ë‚˜ë¬´ëŠ” ë³´í†µ ë‹¤ë¥¸ íšŒê·€ë‚˜ ë¶„ë¥˜ ëª¨ë¸ì— ë¹„í•´ì„œ ì •í™•ë„ê°€ ë–¨ì–´ì§„ë‹¤.



## 2. Exploratory Data Analytics (EDA)

* *(ê°„ë‹¨íˆ ì§šê³  ë„˜ì–´ê°€ê¸°)*

  ### 2-1. ë¬¸ì œ ì •ì˜í•˜ê¸°

  * íƒ€ì´íƒ€ë‹‰ ì¹¨ëª° ì‚¬ê³ ì˜ ì‹¤ì œ ë°ì´í„°ë¥¼ í†µí•´, ìƒì¡´ìœ ë¬´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

  ### 2-2. í›ˆë ¨, í…ŒìŠ¤íŠ¸ ë°ì´í„° í™•ì¸í•˜ê¸°

  * **ë°ì´í„° í™•ì¸**

    ```python
    import numpy as np 
    import pandas as pdhttp://localhost:8888/notebooks/step3%20-%20Classification.ipynb#3-2-2-:-%ED%9B%88%EB%A0%A8,-%ED%85%8C%EC%8A%A4%ED%8A%B8-%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%99%95%EC%9D%B8%ED%95%98%EA%B8%B0
    import matplotlib.pyplot as plt
    import seaborn as sns
    plt.style.use('fivethirtyeight')
    import warnings
    warnings.filterwarnings('ignore')
    %matplotlib inline
    
    data=pd.read_csv('./data/titanic/train.csv')
    
    data.head()
    ```

    ![](D:\soyeon.kim\2. ê°œì¸\images\22.PNG)

  * **ê°’ì´ ì—†ëŠ” ë°ì´í„° í™•ì¸í•˜ê¸°**

    ```python
    data.isnull().sum() #checking for total null values
    ```

    ![](D:\soyeon.kim\2. ê°œì¸\images\23.PNG)
    -> ê´€ì°° ê²°ê³¼: `Age`, `Cabin`, `Embarked` ê°€ null value ë¥¼ ê°€ì§€ê³  ìˆìŒ

  * **ë°ì´í„°ë¡œë¶€í„° 1ì°¨ì ì¸ insight ì•Œì•„ë‚´ê¸°: ìƒì¡´ìëŠ” ëª‡ëª…?**

    ```python
    f,ax=plt.subplots(1,2,figsize=(18,8))
    data['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)
    ax[0].set_title('Survived')
    ax[0].set_ylabel('')
    sns.countplot('Survived',data=data,ax=ax[1])
    ax[1].set_title('Survived')
    plt.show()
    ```

    ![](D:\soyeon.kim\2. ê°œì¸\images\24.PNG)
    -> **ê´€ì°° ê²°ê³¼**

    - ë§ì€ ìŠ¹ê°ì´ Tatanic ì¶©ëŒ ì‚¬ê³ ë¡œë¶€í„° ì‚´ì•„ë‚¨ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.

    - Training Data Setì„ ë³´ë©´, íƒ‘ìŠ¹í•œ 891 ëª…ì˜ ìŠ¹ê° ì¤‘ 350 ëª…ë§Œì´ ì‚´ì•„ ë‚¨ì•˜ìŠµë‹ˆë‹¤. ì¦‰, ì „ì²´ Training Data Setì˜ ìŠ¹ê°ì¤‘ **38.4%** ë§Œì´ ì¶©ëŒì—ì„œ ì‚´ì•„ ë‚¨ì•˜ìŠµë‹ˆë‹¤. ë°ì´í„°ì—ì„œ ë” ë‚˜ì€ í†µì°°ë ¥ì„ ì–»ê³  ì–´ë–¤ ì¹´í…Œê³ ë¦¬ì˜ ìŠ¹ê°ì´ ìƒì¡´í–ˆëŠ”ì§€, ìƒì¡´í•˜ì§€ ëª»í–ˆëŠ”ì§€ë¥¼ ì•Œì•„ ë‚´ê¸° ìœ„í•´ ë” ê¹Šê²Œ ë¶„ì„í•´ì•¼ í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.

      

  ### 2-3. Features ë¶„ì„í•˜ê¸°

  * **Types of Features**

    * **Categorical Features**

      * A categorical variable is one that has two or more categories and each value in that feature can be categorised by them.For example, gender is a categorical variable having two categories (male and female). Now we cannot sort or give any ordering to such variables. They are also known as **Nominal Variables**.
      * ë²”ì£¼í˜• ë³€ìˆ˜(categorical variable)ëŠ” ë‘ ê°œ ì´ìƒì˜ ë²”ì£¼(Category)ê°€ ìˆê³  í•´ë‹¹ featureì˜ ê° ê°’ì„ ë²”ì£¼ë³„ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì„±ë³„(Sex)ì€ ë‘ ë²”ì£¼(ë‚¨ì„±ê³¼ ì—¬ì„±)ê°€ ìˆëŠ” ë²”ì£¼í˜• ë³€ìˆ˜ì…ë‹ˆë‹¤. ë²”ì£¼í˜• ë³€ìˆ˜ëŠ” ì •ë ¬(Sorting)í•˜ê±°ë‚˜ ìˆœì„œë¶€ì—¬(Ordering)ë¥¼ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë²”ì£¼í˜• ë³€ìˆ˜ëŠ” **Nominal Variables(ëª…ëª© ë³€ìˆ˜)** ë¼ê³ ë„ í•©ë‹ˆë‹¤.
      * **Categorical Features in the dataset: Sex,Embarked.**

    * **Ordinal Features**

      * An ordinal variable is similar to categorical values, but the difference between them is that we can have relative ordering or sorting between the values. For eg: If we have a feature like **Height** with values **Tall, Medium, Short**, then Height is a ordinal variable. Here we can have a relative sort in the variable.
      * ìˆœì„œí˜• ë³€ìˆ˜ëŠ” ë²”ì£¼í˜• ë³€ìˆ˜ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ ì°¨ì´ì ì€ ê°’ ì‚¬ì´ì˜ ì •ë ¬(Sorting)í•˜ê±°ë‚˜ ìˆœì„œë¶€ì—¬(Ordering)ë¥¼ í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì˜ˆ : **Height(í‚¤=ì‹ ì¥)** ë³€ìˆ˜ì¸ ê²½ìš°ì— **Tall, Medium, Short** ê°’ì´ ìˆœì„œ(Order)ê°€ ìˆìŠµë‹ˆë‹¤.
      * **Ordinal Features in the dataset: PClass**

    * **Continous Features**

      * A feature is said to be continous if it can take infinate values between any two points or between the minimum or maximum values in the features column.
      * ì—°ì†í˜• ë³€ìˆ˜ëŠ” ì„ì˜ì˜ ë‘ ì  í˜¹ì€ ìµœì†Œê°’ê³¼ ìµœëŒ€ê°’ ì‚¬ì´ì— ë¬´í•œ ê°’ì„ ì·¨í•  ìˆ˜ ìˆëŠ” ê²½ìš°ë¥¼ ë§í•œë‹¤.
      * **Continous Features in the dataset: Age**  

    * **Discrete Features**

      * A feature is said to be discrete if it can take countable values between any two points or between the minimum or maximum values in the features column.

      * ì´ì‚°í˜• ë³€ìˆ˜ëŠ” ì„ì˜ì˜ ë‘ ì  í˜¹ì€ ìµœì†Œê°’ê³¼ ìµœëŒ€ê°’ ì‚¬ì´ì— ìœ í•œ ê°’ì„ ì·¨í•  ìˆ˜ ìˆëŠ” ê²½ìš°ë¥¼ ë§í•œë‹¤.

      * **Discrete Features in the dataset: SibSp, Parch**

        

    ####  2-3-1. Categorical Feature: Sex

    ```python
    f,ax=plt.subplots(1,2,figsize=(18,8))
    data[['Sex','Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])
    ax[0].set_title('Survived vs Sex')
    sns.countplot('Sex',hue='Survived',data=data,ax=ax[1])
    ax[1].set_title('Sex:Survived vs Dead')
    plt.show()
    ```

    ![](D:\soyeon.kim\2. ê°œì¸\images\25.PNG)

    -> ê´€ì°°ê²°ê³¼: ì‚´ì•„ë‚¨ì€ ì—¬ì„±ì˜ ìˆ˜ê°€ ì‚´ì•„ë‚¨ì€ ë‚¨ì„±ì˜ ìˆ˜ì˜ 2ë°° ì •ë„ ë©ë‹ˆë‹¤. ë˜í•œ **ì—¬ì„±ì˜ ìƒì¡´ìœ¨ì€ ê±°ì˜ 75% ì¸ë° ë°˜í•´, ë‚¨ì„±ì˜ ìƒì¡´ìœ¨ì€ ê±°ì˜ 18-19% ì •ë„ ë©ë‹ˆë‹¤.**

    

    #### 2-3-2. Ordinal Feature: Pclass

    ```python
    f,ax=plt.subplots(1,2,figsize=(18,8))
    data['Pclass'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'],ax=ax[0])
    ax[0].set_title('Number Of Passengers By Pclass')
    ax[0].set_ylabel('Count')
    sns.countplot('Pclass',hue='Survived',data=data,ax=ax[1])
    ax[1].set_title('Pclass:Survived vs Dead')
    plt.show()
    ```

    ![](D:\soyeon.kim\2. ê°œì¸\images\26.PNG)

    * **ê´€ì°° ê²°ê³¼**
      * ê·¸ë˜í”„ë¥¼ í†µí•´ Pclass1ì˜ ìŠ¹ê°ë“¤ì´ êµ¬ì¡° ì‘ì—…ì‹œì— ë†’ì€ ìš°ì„  ìˆœìœ„ë¥¼ ë¶€ì—¬ ë°›ì•˜ìŒì„ ìœ ì¶”í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Pclass3ì˜ ìŠ¹ê° ìˆ˜ëŠ” í›¨ì”¬ ë§ì•˜ì§€ë§Œ ìƒì¡´ìœ¨ì€ ì•½ **25%** ë¡œ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤.
      * Pclass1ì€ **63%** ì¸ ë°˜ë©´ Pclass2ëŠ” **48%** ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëˆê³¼ ì§€ìœ„ê°€ ìƒì¡´ìœ¨ì— ì˜í–¥ì„ ì£¼ì—ˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    

    #### 2-3-3. Continous Feature: Age

    ```python
    f,ax=plt.subplots(1,2,figsize=(18,8))
    sns.violinplot("Pclass","Age", hue="Survived", data=data,split=True,ax=ax[0])
    ax[0].set_title('Pclass and Age vs Survived')
    ax[0].set_yticks(range(0,110,10))
    sns.violinplot("Sex","Age", hue="Survived", data=data,split=True,ax=ax[1])
    ax[1].set_title('Sex and Age vs Survived')
    ax[1].set_yticks(range(0,110,10))
    plt.show()
    ```

    ![](D:\soyeon.kim\2. ê°œì¸\images\27.PNG)

    * **ê´€ì°° ê²°ê³¼**

      1) ì–´ë¦°ì´ì˜ ìˆ˜ëŠ” Pclassì˜ ë²ˆí˜¸ê°€ ì¦ê°€ì— ë§ì¶”ì–´ í•¨ê»˜ ì¤‘ê°€í•¨. 10ì„¸ ì´í•˜ ì–´ë¦°ì´ì˜ ìƒì¡´ìœ¨ì´ Pclassì™€ ê´€ê³„ì—†ì´ ì–‘í˜¸í•œ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.

      2) Pclass1ì—ì„œ 20 ì„¸ì—ì„œ 50 ì„¸ ì‚¬ì´ì˜ Passenegersì— ëŒ€í•œ ìƒì¡´ ê¸°íšŒëŠ” ë†’ìœ¼ë©° ì—¬ì„±ì—ê²ŒëŠ” í›¨ì”¬ ë” ì¢‹ìŠµë‹ˆë‹¤.

      3) ë‚¨ì„±ì˜ ê²½ìš° ìƒì¡´ìœ¨ì€ ì—°ë ¹ì´ ì¦ê°€í•¨ì— ë”°ë¼ ê°ì†Œí•©ë‹ˆë‹¤.

    

    #### 2-3-4. Categorical Value: Embarked

    ```python
    sns.factorplot('Embarked','Survived',data=data)
    fig=plt.gcf()
    fig.set_size_inches(5,3)
    plt.show()
    ```

    ![](D:\soyeon.kim\2. ê°œì¸\images\28.PNG)

    * **ê´€ì°°ê²°ê³¼**

      í¬íŠ¸C ì—ì„œ ìŠ¹ì„ í•œ ìŠ¹ê°ë“¤ì˜ ìƒì¡´ìœ¨ì´ ì•½ 0.55ë¡œ ê°€ì¥ ë†’ê³  í¬íŠ¸Sì— ìŠ¹ì„ í•œ ìŠ¹ê°ë“¤ì˜ ìƒì¡´ìœ¨ì´ ê°€ì¥ ë‚®ìŒ.

    

    #### 2-3-5. Discrete Feature: SibSp

    ```python
    f,ax=plt.subplots(1,2,figsize=(20,8))
    sns.barplot('SibSp','Survived',data=data,ax=ax[0])
    ax[0].set_title('SibSp vs Survived')
    sns.factorplot('SibSp','Survived',data=data,ax=ax[1])
    ax[1].set_title('SibSp vs Survived')
    plt.close(2)
    plt.show()
    ```

    ![](D:\soyeon.kim\2. ê°œì¸\images\29.PNG)

    * **ê´€ì°° ê²°ê³¼**
      * barplot ì™€ factorplot ì€ ê°€ì¡±ì—†ì´ í˜¼ì ìŠ¹ì„ í•œ ìŠ¹ê°ì— ëŒ€í•´ì„œ 34.5%ì˜ ìƒì¡´ìœ¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. SibSp ê°€ ì¦ê°€í•˜ë©´ ê·¸ë˜í”„ê°€ ëŒ€ëµì ìœ¼ë¡œ ê°ì†Œí•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ê°€ì¡±ì´ ìˆë‹¤ë©´, ê°€ì¡±ì„ êµ¬í•˜ë ¤ê³  ë” ë…¸ë ¥í•  ê²ƒì…ë‹ˆë‹¤. ë†€ë¼ìš´ ì‚¬ì‹¤ì€ 5-8 ëª…ìœ¼ë¡œ êµ¬ì„±ëœ ê°€ì¡±ì˜ ìƒì¡´ìœ¨ì€ **0%** ì…ë‹ˆë‹¤.

      * ì´ìœ ëŠ” **Pclass** ì— ìˆì—ˆìŠµë‹ˆë‹¤. crosstab ì€ SibSp > 3 ì¸ ìŠ¹ê°ì´ ëª¨ë‘ Pclass3ì— ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. Pclass3(>3)ì¸ ìŠ¹ê°ì€ ëª¨ë‘ ì‚´ì•„ë‚¨ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.

        

    #### 2-3-6. Discrete Feature: Parch

    ```python
    f,ax=plt.subplots(1,2,figsize=(20,8))
    sns.barplot('Parch','Survived',data=data,ax=ax[0])
    ax[0].set_title('Parch vs Survived')
    sns.factorplot('Parch','Survived',data=data,ax=ax[1])
    ax[1].set_title('Parch vs Survived')
    plt.close(2)
    plt.show()
    ```

    ![](D:\soyeon.kim\2. ê°œì¸\images\30.PNG)

    * **ê´€ì°° ê²°ê³¼**

      * SibSp ì™€ ê²°ê³¼ê°€ ë§¤ìš° ë¹„ìŠ·í•©ë‹ˆë‹¤. ê°€ì¡±ì´ í•¨ê»˜ íƒ‘ìŠ¹í•œ ìŠ¹ê°ì€ ìƒì¡´ ê¸°íšŒê°€ ë” í½ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ parch ìˆ«ìê°€ ì˜¬ë¼ê°ì— ë”°ë¼ ê°ì†Œí•©ë‹ˆë‹¤.

      * 1-3 ëª…ì˜ ê°€ì¡±(ë¶€ëª¨ í˜¹ì€ ìì‹)ì„ ê°€ì§„ ìŠ¹ê°ì€ í˜¼ì ìŠ¹ì„ í•œ ìŠ¹ê°ë³´ë‹¤ ìƒì¡´ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤. 4ëª… ì´ìƒì˜ ê°€ì¡±(ë¶€ëª¨ í˜¹ì€ ìì‹)ì„ ê°€ì§„ ìŠ¹ê°ì˜ ìƒì¡´ìœ¨ì€ ë‚®ìŠµë‹ˆë‹¤.

        

    #### 2-3-7. Continous Feature: Fare

    ```python
    f,ax=plt.subplots(1,3,figsize=(38,8))
    sns.distplot(data[data['Pclass']==1].Fare,ax=ax[0])
    ax[0].set_title('Fares in Pclass 1')
    sns.distplot(data[data['Pclass']==2].Fare,ax=ax[1])
    ax[1].set_title('Fares in Pclass 2')
    sns.distplot(data[data['Pclass']==3].Fare,ax=ax[2])
    ax[2].set_title('Fares in Pclass 3')
    plt.show()
    ```

    ![](D:\soyeon.kim\2. ê°œì¸\images\31.PNG)

    * **ê´€ì°° ê²°ê³¼**

      Pclass1 ì˜ ë¶„í¬ê°€ Fare ê°’ìœ¼ë¡œ ë´¤ì„ ë•Œ ê°€ì¥ ë„“ê²Œ ë¶„í¬ë˜ì–´ ìˆìŒ.

      

  * **ì „ì²´ Feature ë“¤ì— ëŒ€í•œ ê´€ì°° ê²°ê³¼**

    * `Sex:` ë‚¨ìë³´ë‹¤ ì—¬ìì¼ ë•Œ ìƒì¡´ìœ¨ì´ ë†’ìŒ.
    * `Pclass:` **1ë“±ì„ ìŠ¹ê°**ì˜ ìƒì¡´ìœ¨ì´ ë§¤ìš° ë†’ìŒ. **Pclass3ì˜ ìƒì¡´ìœ¨ì€ ë§¤ìš° ë‚®ìŒ**. **ì—¬ì„±**ì˜ ê²½ìš° **Pclass1** ì˜ ìƒì¡´ìœ¨ì´ 1ì´ë©° **Pclass2** ì˜ ìƒì¡´ìœ¨ë„ ë†’ìŒ. **ìƒì¡´í•˜ê¸° ìœ„í•´ì„œ ëˆê³¼ ì§€ìœ„ê°€ ë§¤ìš° ì¤‘ìš”í–ˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ë³´ì—¬ì¤Œ.**.
    * `Age` 5-10ì„¸ ë¯¸ë§Œì˜ ì–´ë¦°ì´ëŠ” ìƒì¡´ìœ¨ì´ ë†’ìŒ. 15~35ì„¸ ì‚¬ì´ì˜ ìŠ¹ê°ì´ ë§ì´ ì‚¬ë§í•¨.
    * `Embarked` : í¥ë¯¸ë¡œìš´ ì‚¬ì‹¤ì„ ë³¼ìˆ˜ ìˆìŒ **Pclass1 ìŠ¹ê°ì˜ ëŒ€ë‹¤ìˆ˜ê°€ Sì—ì„œ ì¼ì–´ë‚¬ìŒì—ë„ ë¶ˆêµ¬í•˜ê³  Cì—ì„œì˜ ìƒì¡´ìœ¨ì´ ë” ë†’ìŒ.** Qì—ì„œì˜ ìƒì¡´í•œ ìŠ¹ê°ì€ ëª¨ë‘ **Pclass3** ì—ì„œ ë‚˜ì™”ìŒ.
    * `Parch+SibSp`: 1-2ëª…ì˜ í˜•ì œìë§¤/ë°°ìš°ì í˜¹ì€ 1-3ëª…ì˜ ë¶€ëª¨/ìì‹ê³¼ í•¨ê»˜ ìŠ¹ì„ í•œ ìŠ¹ê°ì´ í˜¼ì ìŠ¹ì„ í–ˆê±°ë‚˜ ëŒ€ê°€ì¡±ê³¼ í•¨ê»˜ ìŠ¹ì„ í•œ ìŠ¹ê°ë³´ë‹¤ ìƒì¡´ìœ¨ì´ ë†’ìŒ.





## 3. Preprocessing

* *ìœ„ì—ì„œ ë³´ì •í•œ ë°ì´í„°ë¥¼ í•œë²ˆ ë” ë³´ì •í•˜ê¸°*

  ### 3-1. Feature Engineering

  * ë‹¤ìˆ˜ì˜ featureë“¤ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ëª¨ë“  featureê°€ ëª¨ë‘ ì¤‘ìš”í•˜ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤. featureë“¤ ì¤‘ì—ëŠ” ì œê±°ë˜ì–´ì•¼ í•˜ëŠ” redundent featureì´ ì¡´ì¬í•˜ê³  ë•Œë¡œëŠ” ë‹¤ë¥¸ featureë“¤ì—ì„œ ë¶€í„° ìƒˆë¡œìš´ featureë¥¼ ì¶”ê°€í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

  * í•œ ì˜ˆë¡œ ì•ì—ì„œ 'Name' featureì—ì„œ 'Initial' featureë¥¼ ë½‘ì•„ë‚´ì–´ ìƒˆë¡œìš´ featureë¡œ ì¶”ê°€í•œ ê²½ìš°ë¥¼ ë³´ì•˜ìŠµë‹ˆë‹¤. ì´ ì²˜ëŸ¼ ìƒˆë¡œìš´ featureë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ëª‡ê°€ì§€ featureëŠ” ì œê±°í•˜ëŠ” ê²ƒì´ í•„ìš”í•©ë‹ˆë‹¤. ê²°êµ­ Feature Engineeringì€ ì£¼ì–´ì§„ Datasetì— ì¡´ì¬í•˜ëŠ” featureë¥¼ ì˜ˆì¸¡ ëª¨ë¸ë§ì— ì í•©í•œ í˜•íƒœì˜ Featureë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì´ë¼ê³  ë³´ì•„ì•¼ í•  ê²ƒì…ë‹ˆë‹¤.

    #### 3-1-1. Age_band

    * **Age Feature ì˜ ë¬¸ì œì **

      - **AgeëŠ” Continous Featureì…ë‹ˆë‹¤**. ì´ê²ƒì´ ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
      - ì˜ˆë“¤ë“¤ì–´ ìŠ¤í¬ì¸  ì„ ìˆ˜ë¥¼ **Sex(ì„±ë³„)** ë¡œ ë¬¶ì–´ë³´ë©´, ì‰½ê²Œ 'ë‚¨'ê³¼ 'ì—¬' ë‘ê°€ì§€ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë¬¶ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
      - ë§Œì•½ ìŠ¤í¬ì¸  ì„ ìˆ˜ë¥¼ **Age** ë¡œ ë¬¶ì–´ë³¸ë‹¤ë©´ ì–´ë–»ê¹Œìš”? ë§Œì•½ 30ëª…ì´ ìˆë‹¤ë©´, ì´ ê²½ìš° 30ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ Age ê°’ì´ ìˆì„ ê²ƒì´ê³  ì´ê²ƒì´ Continous Featureì˜ ë¬¸ì œì ì´ ë©ë‹ˆë‹¤.

    * **Continous FeatureëŠ” Categorical Feature** ë¡œ ë³€ê²½í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. ì´ ë•Œ ì‚¬ìš©ë˜ëŠ” ê¸°ë²•ì´ Binning í˜¹ì€ Normalisation ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” Binningì„ ì‚¬ìš©í•´ ë´…ë‹ˆë‹¤.

    * ìŠ¹ê°ì˜ ë‚˜ì´ì¤‘ ìµœëŒ€ê°’ì´ 80ì´ë¯€ë¡œ... 0-80 êµ¬ê°„ì„ 5ê°œì˜ binìœ¼ë¡œ ë‚˜ëˆ„ì–´ë´…ë‹ˆë‹¤. 80/5=16. í•˜ë‚˜ì˜ binì˜ ì‚¬ì´ì¦ˆëŠ” 16ì´ ë©ë‹ˆë‹¤.

      ```python
      data['Age_band'].value_counts().to_frame().style.background_gradient(cmap='summer')#checking the number of passenegers in each band
      
      sns.factorplot('Age_band','Survived',data=data,col='Pclass')
      plt.show()
      ```

      ![](D:\soyeon.kim\2. ê°œì¸\images\32.PNG)

      * **ê´€ì°° ê²°ê³¼**

        Pclassì™€ ê´€ê³„ì—†ì´ Age_bandê°€ ì¦ê°€í•¨ì— ë”°ë¼ ìƒì¡´ìœ¨ì´ ê°ì†Œí•©ë‹ˆë‹¤.

        

    #### 3-1-2. Family_Size and Alone

    * "Family_size" ì™€ "Alone" ì´ë¼ëŠ” featureë¥¼ ì¶”ê°€í•´ì„œ ë¶„ì„í•´ ë´…ì‹œë‹¤. "Family_size" ì™€ "Alone"ëŠ” "Parch" ê³¼ "SibSp"ë¥¼ ì´ìš©í•´ì„œ ë§Œë“­ë‹ˆë‹¤. "Family_size"ì˜ ê²½ìš°ëŠ” ìƒì¡´ìœ¨ì´ ìŠ¹ê°ì˜ "Family Size"ì™€ ê´€ë ¨ì´ ìˆëŠ”ì§€ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ "Parch" ê³¼ "SibSp"ë¥¼ í•©í•œ featureë¥¼ ìƒˆë¡œ ë§Œë“  ê²ƒì…ë‹ˆë‹¤. "Alone"ì€ ìŠ¹ê°ì´ í˜¼ì ìŠ¹ì„ í–ˆëŠ”ì§€ ì—¬ë¶€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìƒˆë¡œìš´ feature ì…ë‹ˆë‹¤.

      ```python
      data['Family_Size']=0
      data['Family_Size']=data['Parch']+data['SibSp']#family size
      data['Alone']=0
      data.loc[data.Family_Size==0,'Alone']=1#Alone
      
      f,ax=plt.subplots(1,2,figsize=(18,6))
      sns.factorplot('Family_Size','Survived',data=data,ax=ax[0])
      ax[0].set_title('Family_Size vs Survived')
      sns.factorplot('Alone','Survived',data=data,ax=ax[1])
      ax[1].set_title('Alone vs Survived')
      plt.close(2)
      plt.close(3)
      plt.show()
      ```

      ![](D:\soyeon.kim\2. ê°œì¸\images\33.PNG)

      * **ê´€ì°° ê²°ê³¼**

        **Family_Size=0 ëŠ” ê°€ì¡±ì—†ì´ í˜¼ì ìŠ¹ì„ í•œ ìŠ¹ê°ì´ë¼ëŠ” ëœ»ì…ë‹ˆë‹¤.** family_size=0ì¸ ê²½ìš° ìƒì¡´ìœ¨ë‹ˆ ë‚®ìŠµë‹ˆë‹¤. family_size > 4 ì¸ ê²½ìš° ìƒì¡´ìœ¨ì´ ê¸‰ê²©íˆ ë–¨ì–´ì§‘ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ featureë„ ì˜ˆì¸¡ ëª¨ë¸ì— ìœ ì…ë  ìˆ˜ ìˆëŠ” ì¤‘ìš”í•œ featureë¼ê³  ë³´ì—¬ì§‘ë‹ˆë‹¤.

      ```python
      sns.factorplot('Alone','Survived',data=data,hue='Sex',col='Pclass')
      plt.show()
      ```

      ![](D:\soyeon.kim\2. ê°œì¸\images\34.PNG)

      * **ê´€ì°° ê²°ê³¼**

        í˜¼ì ìŠ¹ì„ í•œ ìŠ¹ê°ì€ "Sex", "Pclass"ì™€ ìƒê´€ì—†ì´ ìƒì¡´ìœ¨ì´ ë‚®ìŠµë‹ˆë‹¤. ë‹¨ ì˜ˆì™¸ì ìœ¼ë¡œ Pclass3 ì´ë©´ì„œ ì—¬ì„±ì¸ ìŠ¹ê°ì¸ ê²½ìš°ëŠ” í˜¼ì ìŠ¹ì„ í•œ ìŠ¹ê°ì¸ ê²½ìš°ê°€ ê°€ì¡±ê³¼ í•¨ê»˜ ìŠ¹ì„ í•œ ìŠ¹ê°ë³´ë‹¤ ìƒì¡´ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤.

        

  ### 3-2. Cleaning

  * **Converting String Values into Numeric**: ì¼ë°˜ì ìœ¼ë¡œ...ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ String ê°’ì„ ë„£ì„ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, "Sex", "Embarked" ê°™ì€ ë¬¸ìí˜• featureì„ ìˆ«ìí˜• featureë“¤ë¡œ ë³€í™˜í•´ì•¼ë§Œ í•©ë‹ˆë‹¤.

    ```python
    data['Sex'].replace(['male','female'],[0,1],inplace=True)
    data['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)
    data['Initial'].replace(['Mr','Mrs','Miss','Master','Other'],[0,1,2,3,4],inplace=True)
    ```

  * **Dropping UnNeeded Features**

    * `Name`--> We don't need name feature as it cannot be converted into any categorical value.
    * `Age`--> We have the Age_band feature, so no need of this.
    * `Ticket`--> It is any random string that cannot be categorised.
    * `Fare`--> We have the Fare_cat feature, so unneeded
    * `Cabin`--> A lot of NaN values and also many passengers have multiple cabins. So this is a useless feature.
    * `Fare_Range`--> We have the fare_cat feature.
    * `PassengerId`--> Cannot be categorised.

    ```python
    data.drop(['Name','Age','Ticket','Fare','Cabin','Fare_Range','PassengerId'],axis=1,inplace=True)
    sns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2,annot_kws={'size':20})
    fig=plt.gcf()
    fig.set_size_inches(18,15)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.show()
    ```





## 4. Model Selection

* ì•Œê³ ë¦¬ì¦˜ ëŒë¦¬ê¸°..!

  ### 4-1. Running Basic Algorithms

  * EDAë§Œìœ¼ë¡œ ë¬¸ì œ(Titanic ìŠ¹ê°ë“¤ì˜ ì‚¬ê³ ì—ì„œ ì‚´ì•„ë‚¨ì„ì§€ ì£½ì„ì§€ ì—¬ë¶€ë¥¼ ì˜ˆì¸¡)ì— ëŒ€í•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë†’í ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì—¬ëŸ¬ê°€ì§€ Classification ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ì–´ë–»ê²Œ ë¬¸ì œë¥¼ í’€ìˆ˜ ìˆì„ì§€ ì•Œì•„ë³´ì...

    1. Logistic Regression

    2. Support Vector Machines(Linear and radial)

    3. Random Forest

    4. K-Nearest Neighbours

    5. Naive Bayes

    6. Decision Tree

  * **Classification ì˜ ì¢…ë¥˜**

    1. **íŒë³„í•¨ìˆ˜(Discriminant Function) ëª¨ë¸**

       í•˜ë‚˜ëŠ” ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ì¹´í…Œê³ ë¦¬ì— ë”°ë¼ ì„œë¡œ ë‹¤ë¥¸ ì˜ì—­ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²½ê³„ë©´(decision boundary)ì„ ì°¾ì•„ë‚¸ ë‹¤ìŒ ì´ ê²½ê³„ë©´ìœ¼ë¡œë¶€í„° ì£¼ì–´ì§„ ë°ì´í„°ê°€ ì–´ëŠ ìœ„ì¹˜ì— ìˆëŠ”ì§€ë¥¼ ê³„ì‚°í•˜ëŠ” íŒë³„í•¨ìˆ˜(discriminant function)ë¥¼ ì´ìš©í•˜ëŠ” íŒë³„í•¨ìˆ˜ ëª¨ë¸ì´ë‹¤.

    2. **í™•ë¥ ì  ëª¨ë¸ = í™•ë¥ ì  íŒë³„(Discriminative) ëª¨ë¸ | í™•ë¥ ì  ìƒì„±(Generative) ëª¨ë¸**

       ë˜ ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ì£¼ì–´ì§„ ë°ì´í„°ì— ëŒ€í•´(conditionally) ê° ì¹´í…Œê³ ë¦¬ í˜¹ì€ í´ë˜ìŠ¤ê°€ ì •ë‹µì¼ ì¡°ê±´ë¶€í™•ë¥ (conditional probability)ë¥¼ ê³„ì‚°í•˜ëŠ” í™•ë¥ ì  ëª¨ë¸ì´ë‹¤. ì¡°ê±´ë¶€í™•ë¥  ê¸°ë°˜ ë°©ë²•ì€ ì¡°ê±´ë¶€í™•ë¥ ì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì— ë”°ë¼ ì§ì ‘ ì¡°ê±´ë¶€í™•ë¥  í•¨ìˆ˜ë¥¼ ì¶”ì •í•˜ëŠ” í™•ë¥ ì  íŒë³„(discriminative) ëª¨ë¸ê³¼ ë² ì´ì¦ˆ ì •ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” í™•ë¥ ì  ìƒì„±(generative) ëª¨ë¸ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì§„ë‹¤.

       

  ### 4-2. Cross Validation

  * *ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ë°ì´í„°ëŠ” ë¶ˆê· í˜•í•©ë‹ˆë‹¤. íŠ¹íˆ ë°ì´í„°ì…‘ì˜ í¬ê¸°ê°€ ì‘ì€ ê²½ìš° í…ŒìŠ¤íŠ¸ì…‘ì— ëŒ€í•œ ì„±ëŠ¥ í‰ê°€ì˜ ì‹ ë¢°ì„±ì´ ë–¨ì–´ì§€ê²Œ ë©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ì…‘ì„ ì–´ë–»ê²Œ ì¡ëŠëƒì— ë”°ë¼ ì„±ëŠ¥ì´ ë§¤ìš° ë‹¤ë¥´ê²Œ ì¸¡ì •ë ìˆ˜ ìˆê³  ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Cross Validationì€ ìµœì†Œ í•œ ë²ˆì€ í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ì“°ì´ë„ë¡ í•©ë‹ˆë‹¤. ì•„ë˜ì˜ ê·¸ë¦¼ì„ ë³´ë©´, ë°ì´í„°ë¥¼ 5ê°œë¡œ ìª¼ê°œ ë§¤ë²ˆ í…ŒìŠ¤íŠ¸ì…‹ì„ ë°”ê¿”ë‚˜ê°€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì²« ë²ˆì§¸ Iterationì—ì„œëŠ” BCDEë¥¼ íŠ¸ë ˆì´ë‹ ì…‹ìœ¼ë¡œ, Aë¥¼ í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ì„¤ì •í•œ í›„, ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. ë‘ ë²ˆì§¸ Iterationì—ì„œëŠ” ACDEë¥¼ íŠ¸ì…‹ìœ¼ë¡œ, Bë¥¼ í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œí•˜ì—¬ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë©´ ì´ 5ê°œì˜ ì„±ëŠ¥ í‰ê°€ì§€í‘œê°€ ìƒê¸°ê²Œ ë˜ëŠ”ë°, ë³´í†µ ì´ ê°’ë“¤ì„ í‰ê· ì„ ë‚´ì–´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê²Œ ë©ë‹ˆë‹¤.*

  * **K-Fold Cross Validation**

    * *K-Fold Cross Validationì€ ë¨¼ì € ë°ì´í„° ì§‘í•©ì„ kê°œì˜ ë¶€ë¶„ ì§‘í•©ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤. 2) ë°ì´í„° ì„¸íŠ¸ë¥¼ (k = 5) ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë´…ì‹œë‹¤. ìš°ë¦¬ëŠ” í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 1 ë¶€ë¶„ì„ ì˜ˆì•½í•˜ê³  4 ë¶€ë¶„ì— ê±¸ì³ ì•Œê³ ë¦¬ì¦˜ì„ í›ˆë ¨ì‹œí‚µë‹ˆë‹¤. 3) ê° Iteration ì—ì„œ í…ŒìŠ¤íŠ¸ ë¶€ë¶„ì„ ë³€ê²½í•˜ê³  í•™ìŠµí•˜ëŠ” í”„ë¡œì„¸ìŠ¤ë¥¼ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤. 4) ëª¨ë“  Iterationì´ ëë‚˜ê³  ì´ 5ê°œì˜ ì„±ëŠ¥ í‰ê°€ì§€í‘œê°€ ìƒê¸°ê²Œ ë˜ëŠ”ë°, ë³´í†µ ì´ ê°’ë“¤ì„ í‰ê· ì„ ë‚´ì–´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.*
    * *ê° Iterationì—ì„œ í‰ê°€ëœ ëª¨ë¸ì€ ì–´ë–¤ ê²½ìš°ëŠ” underfit ì–´ë–¤ ê²½ìš°ëŠ” overfit ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë“¤ì— ëŒ€í•œ í‰ê· ì„ êµ¬í•˜ë¯€ë¡œ...Cross Validationì„ í†µí•´ ì¢€ ë” ì¼ë°˜í™”ëœ ëª¨ë¸ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.*

  * **Confustion Matrix**

    * *ì •í™•ë„(Accuracy)ê°€ ê°€ì¥ ì¼ë°˜ì ìœ¼ë¡œ ì“°ì´ëŠ” ì„±ëŠ¥í‰ê°€ ì§€í‘œì´ê³  ì§ê´€ì ìœ¼ë¡œ ì´í•´í•˜ê¸° ì‰½ë‹¤ëŠ” ì¥ì ì´ ìˆì§€ë§Œ...ë°ì´í„°ì˜ ë¶„ê· í˜•ìœ¼ë¡œ ì¸í•´ í´ë˜ìŠ¤ì˜ ë¹„ìœ¨ì´ í•œ ìª½ìœ¼ë¡œ ì¹˜ìš°ì³ ìˆì„ ê²½ìš°ëŠ” í‰ê°€ê°€ ì–´ë µë‹¤.(Imbalanced dataset problem)*

    * *í˜¼ë™í–‰ë ¬(Confusion Matrix)ì€ ì •í™•ë„ë³´ë‹¤ ë” ìì„¸í•œ ì •ë³´ë¥¼ ë³´ì—¬ì¤€ë‹¤.*

      * **Hyper-Parameters Tuning**

      * *ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì€ ë¸”ë™ë°•ìŠ¤ì™€ ê°™ìŠµë‹ˆë‹¤. ì´ ë¸”ë™ë°•ìŠ¤ì—ëŠ” ëª‡ ê°€ì§€ ê¸°ë³¸ ë§¤ê°œë³€ìˆ˜ ê°’ì´ ìˆê³  ë” ë‚˜ì€ ëª¨ë¸ì„ ì–»ê¸° ìœ„í•´ ì¡°ì •í•˜ê±°ë‚˜ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ë“¤ì–´ SVM ëª¨ë¸ì˜ Cì™€ Gammaì™€ ê°™ì€ ê²ƒì…ë‹ˆë‹¤. ì´ê²ƒì„ ìš°ë¦¬ëŠ” hyper-parameterë¼ê³  ë¶€ë¦…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” hyper-parameter ì¡°ì •ì„ í†µí•´ ì•Œê³ ë¦¬ì¦˜ì˜ í•™ìŠµ ì†ë„ë¥¼ ë³€ê²½í•˜ê³  ë” ë‚˜ì€ ëª¨ë¸ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì„ Hyper-Parameter Tuningì´ë¼ê³  í•©ë‹ˆë‹¤.*

      * SVMì™€ RandomForests 2ê°€ì§€ ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ì„œ ê°ê° Hyper-Parameter Tuningì„ ì§„í–‰

        * **SVM**

          ```python
          from sklearn.model_selection import GridSearchCV
          C=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]
          gamma=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]
          kernel=['rbf','linear']
          hyper={'kernel':kernel,'C':C,'gamma':gamma}
          gd=GridSearchCV(estimator=svm.SVC(),param_grid=hyper,verbose=True)
          gd.fit(X,Y)
          print(gd.best_score_)
          print(gd.best_estimator_)
          ```

        * **Random Forests**

          ```python
          n_estimators=range(100,1000,100)
          hyper={'n_estimators':n_estimators}
          gd=GridSearchCV(estimator=RandomForestClassifier(random_state=0),param_grid=hyper,verbose=True)
          gd.fit(X,Y)
          print(gd.best_score_)
          print(gd.best_estimator_)
          ```

        -> Rbf-Svmì— ëŒ€í•œ ìµœê³ ì˜ Accuracy ìŠ¤ì½”ì–´ëŠ” **82.82% ì´ê³  ì´ë•Œ Hyper-ParameterëŠ” C=0.05 ì´ê³  gamma=0.1** ì´ë‹¤.

        -> RandomForestì— ëŒ€í•œ ìµœê³ ì˜ Accuracy ìŠ¤ì½”ì–´ëŠ” **81.8% ì´ê³  ì´ë•Œ Hyper-ParameterëŠ” n_estimators=900**.





## 5. Model Evaluation & Prediction

* ë°ì´í„° ì„±ëŠ¥ ë†’ì´ê¸°

  ### 5-1. Ensembling

  * ì•™ìƒë¸”(Ensemble) í•™ìŠµì€ ì—¬ëŸ¬ ê°œì˜ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ê³ , ê·¸ ì˜ˆì¸¡ì„ ê²°í•©í•¨ìœ¼ë¡œì¨ ë³´ë‹¤ ì •í™•í•œ ìµœì¢… ì˜ˆì¸¡ì„ ë„ì¶œí•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. í•˜ë‚˜ì˜ ê°•í•œ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ ì—¬ëŸ¬ ê°œì˜ ì•½í•œ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì´ ë‚«ë‹¤ ë¼ëŠ” ì•„ì´ë””ì–´ë¥¼ ê°€ì§€ê³  ì´í•´í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤.

  * ì´ë¯¸ì§€, ì˜ìƒ, ìŒì„± ë“±ì˜ ë¹„ì •í˜• ë°ì´í„°ì˜ ë¶„ë¥˜ëŠ” ë”¥ëŸ¬ë‹ì´ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆì§€ë§Œ, ëŒ€ë¶€ë¶„ì˜ ì •í˜• ë°ì´í„° ë¶„ë¥˜ ì‹œì—ëŠ” ì•™ìƒë¸”ì´ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚´ê³  ìˆìŠµë‹ˆë‹¤.

  * ë¬¸ì œì™€ ë°ì´í„°ì— ë”°ë¼ ë‹¨ì¼ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë” ì¢‹ì€ ê²½ìš°ë„ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì•™ìƒë¸” ê¸°ë²•ì„ ì‚¬ìš©í•˜ë©´ ë” ìœ ì—°ì„±ìˆëŠ” ëª¨ë¸ì„ ë§Œë“¤ë©° ë” ì¢‹ì€ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  * Classificationì—ì„œ ì•™ìƒë¸” í•™ìŠµì˜ ìœ í˜•ì€ ê°€ì¥ ë§ì´ ì•Œë ¤ì§„ Voting, Bagging, Boosting, Stacking ë“±ì´ ìˆìœ¼ë©°, ê·¸ ì™¸ì—ë„ ë‹¤ì–‘í•œ ì•™ìƒë¸” í•™ìŠµì˜ ìœ í˜•ì´ ìˆìŠµë‹ˆë‹¤.

    #### 5-1-1. Voting

    * ë³´íŒ…(Voting) ë˜ëŠ” ì—ë²„ë¦¬ì§•(Averaging) ì€ ê°€ì¥ ì‰¬ìš´ ì•™ìƒë¸” ê¸°ë²•ì…ë‹ˆë‹¤. ì´ ë‘˜ì€ ì„œë¡œ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ì„ ê°€ì§„ ë¶„ë¥˜ê¸°ë¥¼ ê²°í•©í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

    * ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ìœ¼ë¡œ ì§„í–‰ì´ ë©ë‹ˆë‹¤.

      1. ì¼ì • ìˆ˜ì˜ base modelê³¼ predictë¥¼ ë§Œë“­ë‹ˆë‹¤.
         - Training ë°ì´í„°ë¥¼ ë‚˜ëˆ„ì–´ ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ê±°ë‚˜
         - Training ë°ì´í„°ëŠ” ê°™ì§€ë§Œ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ê±°ë‚˜
      2. Votingì„ ì§„í–‰í•©ë‹ˆë‹¤.
         - Majority Voting (Hard Voting) : ê° ëª¨ë¸ì€ test ë°ì´í„°ì…‹(ë˜ëŠ” ì¸ìŠ¤í„´ìŠ¤)ì˜ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì˜ˆì¸¡ê°’ë“¤ì˜ ë‹¤ìˆ˜ê²°ë¡œ ì˜ˆì¸¡ê°’ì„ ì •í•©ë‹ˆë‹¤.
         - Weighted Voting (Soft Voting) : ê° ëª¨ë¸ì€ Test ë°ì´í„°ì…‹ì˜ ê²°ê³¼ ê°€ëŠ¥ì„±ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ ê°€ëŠ¥ì„±ì— íŠ¹ì • ì—°ì‚°ì„ í•˜ì—¬ ë¶„ë¥˜ labelì˜ í™•ë¥ ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì—ì„œ ê°€ì¤‘ì¹˜ì˜ ì—°ì‚°ì€ ì›í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•  ìˆ˜ ìˆê³ , ë³´í†µ í‰ê· ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ Majority Votingë³´ë‹¤ ìœ ì—°í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìœ¼ë©°, ì˜ˆì¸¡ ì„±ëŠ¥ì´ ì¢‹ì•„ ë” ë§ì´ ì‚¬ìš©í•©ë‹ˆë‹¤.

      ```python
      from sklearn.ensemble import VotingClassifier
      ensemble_lin_rbf=VotingClassifier(estimators=[('KNN',KNeighborsClassifier(n_neighbors=10)),
                                                    ('RBF',svm.SVC(probability=True,kernel='rbf',C=0.5,gamma=0.1)),
                                                    ('RFor',RandomForestClassifier(n_estimators=500,random_state=0)),
                                                    ('LR',LogisticRegression(C=0.05)),
                                                    ('DT',DecisionTreeClassifier(random_state=0)),
                                                    ('NB',GaussianNB()),
                                                    ('svm',svm.SVC(kernel='linear',probability=True))
                                                   ], 
                             voting='soft').fit(train_X,train_Y)
      print('The accuracy for ensembled model is:',ensemble_lin_rbf.score(test_X,test_Y))
      cross=cross_val_score(ensemble_lin_rbf,X,Y, cv = 10,scoring = "accuracy")
      print('The cross validated score is',cross.mean())
      ```

    #### 5-1-2. Bagging

    * ë°°ê¹…(Bagging) ì€ Bootstrap Aggregatingì˜ ì•½ìì…ë‹ˆë‹¤. ë°°ê¹…ì˜ í•µì‹¬ì€ í‰ê· ì„ í†µí•´ ë¶„ì‚°(variance)ê°’ì„ ì¤„ì—¬ ëª¨ë¸ì„ ë” ì¼ë°˜í™”ì‹œí‚¨ë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ë°°ê¹…ì€ Votingê³¼ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ ì§„í–‰ì´ ë©ë‹ˆë‹¤. ì •í™•íˆëŠ” ìµœì¢…ì ìœ¼ë¡œëŠ” Votingì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

      - ì¼ì • ìˆ˜ì˜ base modelì„ ë§Œë“­ë‹ˆë‹¤.

      - ëª¨ë¸ë“¤ì˜ ì•Œê³ ë¦¬ì¦˜ì€ ëª¨ë‘ ê°™ìŠµë‹ˆë‹¤.

      - ê°ê°ì˜ ëª¨ë¸ì€ í›ˆë ¨ë°ì´í„°ì…‹ì—ì„œ ëœë¤ìœ¼ë¡œ ë§Œë“  ì„œë¸Œ ë°ì´í„°ì…‹ì„ ê°ê° ì‚¬ìš©í•©ë‹ˆë‹¤.

      - ì„œë¸Œ ë°ì´í„°ì…‹ì„ ë§Œë“œëŠ” ê³¼ì •ì„ ë¶€íŠ¸ìŠ¤íŠ¸ë˜í•‘(Bootstrapping) ë¶„í•  ë°©ì‹ì´ë¼ê³  í•©ë‹ˆë‹¤. ê°ê°ì˜ ì„œë¸Œ ë°ì´í„°ì…‹ì€ ì¤‘ì²©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

      - ë°°ê¹…ì˜ ê²½ìš°ì—ëŠ” ë°ì´í„° ìƒì„±ê³¼ í›ˆë ¨ì´ ê°œë³„ ëª¨ë¸ì—ì„œ ì§„í–‰ë˜ë¯€ë¡œ, ë³‘ë ¬ ì—°ì‚°ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

      - **Bagged KNN**

        ë°°ê¹…(Bagging)ì€ high varianceì„ ê°€ì§„ ëª¨ë¸ì—ì„œ ì˜ ì‘ë™í•©ë‹ˆë‹¤. ì˜ˆë¥¼ë“¤ì–´ Decision Tree ì…ë‹ˆë‹¤. ë˜í•œ ì‘ì€ **n_neighbours** ê°’ì„ ê°€ì§„ KNN ì—ì„œë„ ì‚¬ìš©ê°€ëŠ¥í•©ë‹ˆë‹¤.

        ```python
        from sklearn.ensemble import BaggingClassifier
        model=BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3),random_state=0,n_estimators=700)
        model.fit(train_X,train_Y)
        prediction=model.predict(test_X)
        print('The accuracy for bagged KNN is:',metrics.accuracy_score(prediction,test_Y))
        result=cross_val_score(model,X,Y,cv=10,scoring='accuracy')
        print('The cross validated score for bagged KNN is:',result.mean())
        ```

      - **Bagged Decision Tree**

        ```python
        model=BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=0,n_estimators=100)
        model.fit(train_X,train_Y)
        prediction=model.predict(test_X)
        print('The accuracy for bagged Decision Tree is:',metrics.accuracy_score(prediction,test_Y))
        result=cross_val_score(model,X,Y,cv=10,scoring='accuracy')
        print('The cross validated score for bagged Decision Tree is:',result.mean())
        ```

    #### 5-1-3. Boosting

    * ë¶€ìŠ¤íŒ…(Boosting) ì•Œê³ ë¦¬ì¦˜ì€ ì—¬ëŸ¬ ê°œì˜ ì•½í•œ í•™ìŠµê¸°(weak learner = estimater)ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµ-ì˜ˆì¸¡í•˜ë©° ì˜ëª» ì˜ˆì¸¡í•œ ë°ì´í„°ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬ë¥¼ í†µí•´ ì˜¤ë¥˜ë¥¼ ê°œì„ í•´ ë‚˜ê°€ë©´ì„œ í•™ìŠµí•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.
    * ê³„ì†í•´ì„œ ë¶„ë¥˜ê¸°ì—ê²Œ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ìŠ¤íŒ…í•˜ë©´ì„œ í•™ìŠµì„ ì§„í–‰í•˜ê¸°ì— ë¶€ìŠ¤íŒ… ë°©ì‹ìœ¼ë¡œ ë¶ˆë¦½ë‹ˆë‹¤.
    * ê¸°ì¡´ Boosting ë°©ë²•ì€ ìˆœì°¨ì ì¸ ì—°ì‚°ì´ í•„ìˆ˜ì ì´ë¯€ë¡œ ë³‘ë ¬ ì—°ì‚°ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ê·¸ë ‡ê¸°ì— ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ì—ì„œëŠ” í•™ìŠµ ì‹œê°„ì´ ë§¤ìš° ë§ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    * ë¶€ìŠ¤íŒ…ì˜ ëŒ€í‘œì ì¸ ì•Œê³ ë¦¬ì¦˜ì€ AdaBoost ì™€ Gradient Boost ê°€ ìˆê³ , ìµœê·¼ ì„±ëŠ¥ë©´ì—ì„œ ì¸ì •ì„ ë°›ì•„ ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” ë¶€ìŠ¤íŒ… ê³„ì—´ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ XGBoost ì™€ LightGBM ì´ ìˆìŠµë‹ˆë‹¤. ê·¸ ì™¸ì—ë„ CatBoostì™€ ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì´ ìˆìŠµë‹ˆë‹¤.
    * **AdaBoost(Adaptive Boosting)**

    * **Stochastic Gradient Boosting**

    * **XGBoost**

    * **LightGBM(Light Gradient Boosting Methods)**

      

    #### 5-1-4. Stacking

    * stacked generalization ìœ¼ë¡œ ì•Œë ¤ì§„ ê¸°ë²•ì…ë‹ˆë‹¤.

    * í˜„ì‹¤ ëª¨ë¸ì— ì ìš©í•˜ëŠ” ê²½ìš°ëŠ” ì ìœ¼ë‚˜, KaggleëŒ€íšŒì—ì„œ ë†’ì€ ìˆœìœ„ë¥¼ ì–»ê¸° ìœ„í•´ ë§ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.

    * ê°€ì¥ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í›ˆë ¨ ë°ì´í„°ì…‹ì„ í†µí•´ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì„ ë§Œë“¤ê³ , ì´ë¥¼ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ë‹¤ì‹œ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ëŒë¦¬ëŠ” ê²ƒì…ë‹ˆë‹¤. ë³´í†µì€ ì„œë¡œ ë‹¤ë¥¸ íƒ€ì…ì˜ ëª¨ë¸ë“¤ ì„ ê²°í•©í•©ë‹ˆë‹¤.

    * ìŠ¤íƒœí‚¹ì—ëŠ” ì´ 2ê°€ì§€ ì¢…ë¥˜ì˜ ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤.
      * ê°œë³„ì ì¸ ê¸°ë°˜ ëª¨ë¸ : ì„±ëŠ¥ì´ ë¹„ìŠ·í•œ ì—¬ëŸ¬ ê°œì˜ ëª¨ë¸
      * ìµœì¢… ë©”íƒ€ ëª¨ë¸ : ê¸°ë°˜ ëª¨ë¸ì´ ë§Œë“  ì˜ˆì¸¡ ë°ì´í„°ë¥¼ í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©í•  ìµœì¢… ëª¨ë¸

    * ì—¬ëŸ¬ ê°œì˜ ê°œë³„ ëª¨ë¸ë“¤ì´ ìƒì„±í•œ ì˜ˆì¸¡ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ë©”íƒ€ ëª¨ë¸ì´ í•™ìŠµí•  ë³„ë„ì˜ í•™ìŠµ ë°ì´í„° ì„¸íŠ¸ì™€ ì˜ˆì¸¡í•  í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì¬ ìƒì„±í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.

    * ëª¨ë¸ì„ í†µí•´ inputì„ ë§Œë“¤ê³ , ë‹¤ì‹œ ëª¨ë¸ì— ë„£ëŠ” êµ¬ì¡°ë•Œë¬¸ì— meta-model ì´ë¼ê³ ë„ ë¶€ë¦…ë‹ˆë‹¤.

    

    #### 5-1-5. Hyper-Parameter Tuning for AdaBoost

    * AdaBoost ì•Œê³ ë¦¬ì¦˜ì´ ê°€ì¥ ë†’ì€ Accuracyë¥¼ ê°€ì§„ Classifierë¡œ ì„ íƒë˜ì—ˆìœ¼ë‹ˆ Hyper-Parameter Tuning ì„ í†µí•´ì„œ ì¡°ê¸ˆ ë” ì„±ëŠ¥ ë†’í˜€ë³´ê¸°

      ```python
      n_estimators=list(range(100,1100,100))
      learn_rate=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]
      hyper={'n_estimators':n_estimators,'learning_rate':learn_rate}
      gd=GridSearchCV(estimator=AdaBoostClassifier(),param_grid=hyper,verbose=True)
      gd.fit(X,Y)
      print(gd.best_score_)
      print(gd.best_estimator_)
      ```

      

      

    ### 5-2. Feature Importance

  * ì–´ë–¤ ë³€ìˆ˜ê°€ ê°€ì¥ ì˜í–¥ì„ ë§ì´ ì£¼ì—ˆë‚˜?

    ```python
    f,ax=plt.subplots(2,2,figsize=(15,12))
    model=RandomForestClassifier(n_estimators=500,random_state=0)
    model.fit(X,Y)
    pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,0])
    ax[0,0].set_title('Feature Importance in Random Forests')
    model=AdaBoostClassifier(n_estimators=200,learning_rate=0.05,random_state=0)
    model.fit(X,Y)
    pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,1],color='#ddff11')
    ax[0,1].set_title('Feature Importance in AdaBoost')
    model=GradientBoostingClassifier(n_estimators=500,learning_rate=0.1,random_state=0)
    model.fit(X,Y)
    pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,0],cmap='RdYlGn_r')
    ax[1,0].set_title('Feature Importance in Gradient Boosting')
    model=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)
    model.fit(X,Y)
    pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,1],color='#FD0F00')
    ax[1,1].set_title('Feature Importance in XgBoost')
    plt.show()
    ```

    ![](D:\soyeon.kim\2. ê°œì¸\images\35.PNG)

    * **ê´€ì°° ê²°ê³¼**

      1. ê³µí†µì ì¸ important featureëŠ” 'Initial', 'Fare_cat', 'Pclass', 'Family_Size' ì…ë‹ˆë‹¤.

      2. 'Sex' feature ì¤‘ìš”ì„±ì´ ë†’ì§€ ì•Šì•„ë³´ì´ëŠ”ë°, 'Sex'ëŠ” 'Pclass'ì™€ í•¨ê»˜ ê³ ë ¤ë˜ì—ˆì„ ë•Œ ë§¤ìš° ì¢‹ì€ ì°¨ë³„ì ì¸ featureë¡œ ê´€ì°°ë˜ì—ˆì—ˆë˜ ê²°ê³¼ë¡œ ë³´ì•˜ì„ ë•Œ ì•½ê°„ ë†€ëìŠµë‹ˆë‹¤. 'Sex' featureëŠ” Random Forestsì—ì„œë§Œ ì¤‘ìš”í•´ ë³´ì…ë‹ˆë‹¤.

         'Initial'ì´ ëª¨ë“  Classifierì˜ ìƒìœ„ Importance featureë¡œ ì˜¬ë¼ì™€ ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆëŠ”ë°, ì´ê²ƒì€ 'Sex'ì™€ 'Initial' ê°„ì— Positive Relationì´ ìˆìŒì´ ì´ë¯¸ ê´€ì°°ë˜ì—ˆê¸° ë•Œë¬¸ì— 'Initial'ê³¼ 'Sex' feature ëŠ” ëª¨ë‘ ì˜ë¯¸ì ìœ¼ë¡œ ì„±ë³„ì„ ë‚˜íƒ€ë‚¸ë‹¤ê³  ë´ì•¼ í•©ë‹ˆë‹¤.

      3. 'Pclass'ì™€ 'Fare_cat'ì€ ì˜ë¯¸ì ìœ¼ë¡œ ìŠ¹ê°ì˜ ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ê³ , 'Family_Size'ëŠ” 'Alone', 'Parch' ë° 'SibSp'ì™€ ê°™ì€ ì˜ë¯¸ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤ê³  ë´ì•¼ í•©ë‹ˆë‹¤.

  

  ### 5-3. Prediction

   * Making Prediction Result into CSV File

     ```python
     result = pd.DataFrame({
             "PassengerId": test_X.index,
             "Survived": Y_pred
         })
     result.to_csv('./data/titanic/result.csv', index=False)
     ```

