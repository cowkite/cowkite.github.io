---
title: "Data Science - Regression (ì„ í˜• íšŒê·€)"
date: 2019-08-28 10:18
categories: development python data-science
---



- Regressionì„ ì´ìš©í•œ ë°ì´í„° ë¶„ì„ì˜ ì „ì²´ workflow
- ML - Regression ì•Œê³ ë¦¬ì¦˜
- Ames ì§€ì—­ì˜ ì§‘ ê°’ì„ ì˜ˆì¸¡í•´ë³´ì (https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview)

## 1. Regression Overview

- ë³´ìŠ¤í„´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ë³´ìŠ¤í„´ ì§‘ê°’ êµ¬í•˜ëŠ” ë¡œì§ì€ ê°€ì¥ ê¸°ì¡´ì ì¸ ì˜ˆì œë¼ì„œ `sklearn` íŒ¨í‚¤ì§€ì— í¬í•¨ì´ ë˜ì–´ìˆìŒ)

  ```python
  from sklearn.datasets import load_boston
  boston = load_boston()
  ```

- Import pandas í›„ DataFrameì— ë„£ì–´ì¤Œ

  ```python
  import pandas as pd
  data = pd.DataFrame(boston.data, columns=boston.feature_names)
  data['MEDV'] = boston.target
  ```

- 25%ëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°

  ```python
  from sklearn.model_selection import train_test_split
  train_data, test_data = train_test_split(data, test_size=0.25)
  ```

- ë°ì´í„°ë¥¼ ìƒˆë¡œìš´ íŒŒìƒë³€ìˆ˜ì— ë„£ì–´ì¤€ë‹¤

  ```python
  RM_mean = train_data.RM.mean()
  RM_std = train_data.RM.std()
  
  MEDV_mean = train_data.MEDV.mean()
  MEDV_std = train_data.MEDV.std()
  
  processed_data = (train_data
                    .assign(RM_standard = lambda x: (x.RM - RM_mean)/RM_std)
                    .assign(MEDV_standard = lambda x: (x.MEDV - MEDV_mean)/MEDV_std)
                   )
  processed_data.head()
  ```

- í…Œì´ë¸” ê²°ê³¼ í™•ì¸

  ![](https://github.com/cowkite/cowkite.github.io/blob/master/_posts/assets/images/01.PNG?raw=true)

- ê·¸ë˜í”„ ê·¸ë¦¬ê¸°

  ```python
  %matplotlib inline
  from matplotlib import pyplot as plt
  import numpy as np
  ```

  

  ### 1-1. ì„ í˜• íšŒê·€

  - ğœƒë¥¼ ì§ì ‘ ì •í•˜ì§€ ì•Šê³  ë¨¸ì‹  ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³¼ ê²ƒì´ë‹¤. ì´ ê²½ìš° ê°€ì¥ ë§ì´ ì“°ì´ëŠ” ì•Œê³ ë¦¬ì¦˜ì€ ì„ í˜• íšŒê·€(linear regression)ì´ë‹¤.

  - ì„¤ëª…ë³€ìˆ˜(explanatory variable)ì™€ ëª©ì ë³€ìˆ˜(target variable) ì‚¬ì´ì— ì„ í˜• ê´€ê³„ê°€ ìˆë‹¤.

    - ë…ë¦½ë³€ìˆ˜(independent variable) = ì„¤ëª…ë³€ìˆ˜(explanatory variable) = ì…ë ¥ê°’ì´ë‚˜ ì›ì¸ì„ ë‚˜íƒ€ë‚¸ë‹¤.
    - ì¢…ì†ë³€ìˆ˜(dependent variable) = ëª©ì ë³€ìˆ˜(target variable) = ê²°ê³¼ê°’ì´ë‚˜ íš¨ê³¼ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.

  - **ê°€ì •**: RM(ë°©ì˜ ê°¯ìˆ˜)ì™€ MEDV(ì§‘ê°’) ì‚¬ì´ì— ì„ í˜• ê´€ê³„ê°€ ìˆë‹¤.

    ```python
    def plot_h(theta_1):
        x = np.linspace(-3, 3)
        y_hat = theta_1 * x
        plt.scatter(x=processed_data.RM_standard, y=processed_data.MEDV_standard)
        plt.plot(x, y_hat, 'r');
    ```

    #### 1-1-1. Cost function

    - ì¢‹ì€ ğœƒÎ¸ë¥¼ ì°¾ê¸° ìœ„í•´ì„œëŠ”, ì–´ë–¤ ğœƒÎ¸ê°€ ì¢‹ì€ ğœƒÎ¸ì¸ì§€ í‰ê°€í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤. Cost functionì€ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ í‹€ë ¸ëŠ”ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤. Loss function, Object functionì´ë¼ê³  ë¶ˆë¦¬ë©° ğ½(ğœƒ)J(Î¸)ë¡œ í‘œê¸°í•œë‹¤.

    - ğ½(ğœƒ)=12ğ‘›âˆ‘ğ‘–=1ğ‘›(â„ğœƒ(ğ‘¥(ğ‘–))âˆ’ğ‘¦(ğ‘–))2

      ```pythond
      def RSS(theta_1):
          y_hat = theta_1 * processed_data.RM_standard
          residual = processed_data.MEDV_standard - y_hat
          n = len(residual)
          rss = 1/(2*n) * (residual ** 2).sum()
          return rss
      ```

    #### 1-1-2. â„ğœƒ(ğ‘¥)hÎ¸(x)ì™€ ğ½(ğœƒ)J(Î¸)ì˜ ì°¨ì´

    - â„ğœƒ(ğ‘¥)hÎ¸(x)ì˜ í•¨ìˆ˜ê°’ì€ ğœƒÎ¸ê°€ ê³ ì •ë˜ì–´ ìˆê³  ğ‘¥x ë”°ë¼ ë³€í•œë‹¤. ì´ì— ë¹„í•´ ğ½(ğœƒ)J(Î¸)ëŠ” ğ‘¥xì™€ ğ‘¦yê°€ ì£¼ì–´ì§„ ë°ì´í„°ë¡œ ê³ ì •ë˜ì–´ ìˆê³  ğœƒÎ¸ì— ë”°ë¼ ê°’ì´ ë³€í•œë‹¤.

    - **ğ½(ğœƒ)J(Î¸)ì˜ ê·¸ë˜í”„**

      - ğœƒÎ¸ì— ë”°ë¼ ë³€í•˜ëŠ” ğ½(ğœƒ)J(Î¸)ì˜ ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

        ```python
        thetas = np.linspace(-3, 3)
        plt.plot(thetas, [RSS(theta) for theta in thetas]);
        ```

        ![](https://github.com/cowkite/cowkite.github.io/blob/master/_posts/assets/images/02.PNG?raw=true)

        â€‹	-> 0.7 ~ 0.8 ì‚¬ì´ì—ì„œ ìµœì†Œê°’ì´ ìˆë‹¤ëŠ” ê²ƒì„ ì§ê´€ì ìœ¼ë¡œ ì•Œ ìˆ˜ ìˆë‹¤.

        - validation ë°ì´í„°ë¥¼ í†µí•´ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ êµ¬í•˜ê³ , training ë°ì´í„°ë¥¼ í†µí•´ íŒŒë¼ë¯¸í„°ë¥¼ êµ¬í•œë‹¤.

        - ìœ„ ê·¸ë˜í”„ì—ì„œ ì •í™•í•œ ìµœì†Ÿê°’ì„ êµ¬í•˜ê¸° ìœ„í•´ì„œëŠ” ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)ì„ ì‚¬ìš©í•´ì•¼í•œë‹¤.

          

    #### 1-1-3. Gradient Descent

    - ğ½(ğœƒ)J(Î¸)ë¥¼ ê·¹ì†Œí™”í•˜ëŠ” ğœƒÎ¸ë¥¼ ìë™ìœ¼ë¡œ ì°¾ì•„ì£¼ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ ê²½ì‚¬ í•˜ê°•ë²• (Gradient Descent)ì´ë‹¤.

    - **ğœƒğ‘—:=ğœƒğ‘—âˆ’ğ›¼âˆ‚âˆ‚ğœƒğ‘—ğ½(ğœƒ)**

      - ğ›¼Î± : í•™ìŠµ ì†ë„ (learning rate) 
        - ì–¼ë§Œí¼ ê°ˆ ê²ƒì¸ì§€: ì´ê±¸ ëª‡ìœ¼ë¡œí•´ì•¼ (ì–¼ë§ˆë‚˜ ì‘ê²Œì¤˜ì•¼) ë°ì´í„°ê°€ ìµœì†Ÿê°’ì— ìˆ˜ë ´í• ì§€ ê³ ë¯¼í•˜ëŠ” ê°’ì„
        - ê²½ì‚¬ í•˜ê°•ë²•ì˜ í•˜ê°• ì†ë„ë¥¼ ì¡°ì ˆí•˜ëŠ” ì—­í• ì„ í•œë‹¤.
        - ì´ ê°’ì€ ë¶„ì„ê°€ê°€ ì§ì ‘ ì§€ì •í•´ì£¼ëŠ” `hyperparameter`ì´ë‹¤.
          - ğ›¼Î±ê°€ ë„ˆë¬´ í´ ë•Œ : ê²½ì‚¬ í•˜ê°•ë²•ì´ ìˆ˜ë ´í•˜ì§€ ëª»í•˜ê³  ë°œì‚°í•˜ê±°ë‚˜ ì§„ë™í•œë‹¤.
          - ğ›¼Î±ê°€ ë„ˆë¬´ ì‘ì„ ë•Œ : ê²½ì‚¬ í•˜ê°•ë²•ì´ ìˆ˜ë ´í•˜ëŠ” ë° ë„ˆë¬´ ê¸´ ì‹œê°„ì´ ê±¸ë¦°ë‹¤. (=í•™ìŠµ ì†ë„ê°€ ëŠë ¤ì§„ë‹¤.)
      - âˆ‚âˆ‚ğœƒğ‘—ğ½(ğœƒ)âˆ‚âˆ‚Î¸jJ(Î¸) : ê¸°ìš¸ê¸° (derivative) -> ë°©í–¥

    - **ê·¹ì†Œì (Local Minimum)ì— ë¹ ì§ˆ ìœ„í—˜**

      - ê²½ì‚¬ í•˜ê°•ë²•ì€ ìµœì†Ÿê°’ì„ ì°¾ëŠ” ì ˆëŒ€ì ì¸ ì•Œê³ ë¦¬ì¦˜ì€ ì•„ë‹ˆë‹¤. ìµœì†Ÿê°’ì´ ì•„ë‹Œ ê·¹ì†Œì  (local minimum), ì¦‰ ì£¼ë³€ë³´ë‹¤ëŠ” ë‚®ì§€ë§Œ ê°€ì¥ ë‚®ì€ ê°’ì€ ì•„ë‹Œ ì§€ì ì— ë¹ ì§ˆ ìœ„í—˜ì´ ìˆë‹¤.![](https://github.com/cowkite/cowkite.github.io/blob/master/_posts/assets/images/03.PNG?raw=true)
      - í•´ê²°ì„ ìœ„í•´ì„  ì•ŒíŒŒê°’ì„ í¬ê²Œ ì¡ê³ , ì ì  ì¤„ì—¬ë‚˜ê°€ëŠ” ë°©ì‹ì„ ì‚¬ìš©
      - ìˆ˜í•™ì ìœ¼ë¡œ ì—­í–‰ë ¬ì„ ì‚¬ìš©í•˜ë©´ thetaë¥¼ êµ¬í•  ìˆ˜ ìˆìœ¼ë‚˜(ğœƒ=(ğ‘‹ğ‘‡ğ‘‹)âˆ’1ğ‘‹ğ‘‡ğ‘¦), ì—­í–‰ë ¬ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆìŒ.
        - ì—­í–‰ë ¬ (inverse matrix) ì—°ì‚°ì€ ëŠë¦¬ë‹¤.
        - (ğ‘‹ğ‘‡ğ‘‹)(XTX)ì˜ ì—­í–‰ë ¬ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.
          - ì“¸ëª¨ ì—†ëŠ” (ì„ í˜•ì¢…ì†ì¸) ë³€ìˆ˜ë“¤
          - ë³€ìˆ˜ë“¤ì´ ë„ˆë¬´ ë§ì„ ë•Œ (ğ‘›<ğ‘šn<m)

    - **ì„ í˜• íšŒê·€ë¥¼ ìœ„í•œ ê²½ì‚¬ í•˜ê°•ë²• (Gradient descent for linear regression)**

      â„ğœƒ(ğ‘¥)=ğœƒ1ğ‘¥hÎ¸(x)=Î¸1x

      ğ½(ğœƒ)=ğ½(ğœƒ1)=12ğ‘›âˆ‘ğ‘–=1ğ‘›(â„ğœƒ(ğ‘¥(ğ‘–))âˆ’ğ‘¦(ğ‘–))2=12ğ‘›âˆ‘ğ‘–=1ğ‘›(ğœƒ1ğ‘¥(ğ‘–)âˆ’ğ‘¦(ğ‘–))2J(Î¸)=J(Î¸1)=12nâˆ‘i=1n(hÎ¸(x(i))âˆ’y(i))2=12nâˆ‘i=1n(Î¸1x(i)âˆ’y(i))2

      âˆ‚âˆ‚ğœƒ1ğ½(ğœƒ1)=1ğ‘›âˆ‘ğ‘–=1ğ‘›(ğœƒ1ğ‘¥(ğ‘–)âˆ’ğ‘¦(ğ‘–))ğ‘¥(ğ‘–)âˆ‚âˆ‚Î¸1J(Î¸1)=1nâˆ‘i=1n(Î¸1x(i)âˆ’y(i))x(i)

      ğœƒ1:=ğœƒ1âˆ’ğ›¼âˆ‚âˆ‚ğœƒğ‘—ğ½(ğœƒ)=ğœƒ1âˆ’ğ›¼1ğ‘›âˆ‘ğ‘–=1ğ‘›(ğœƒ1ğ‘¥(ğ‘–)âˆ’ğ‘¦(ğ‘–))ğ‘¥(ğ‘–)

      ```python
      x = processed_data.RM_standard
      y = processed_data.MEDV_standard
      n = len(x)
      alpha = 0.3
      theta_1 = initial_theta_1 = -0.2
      
      ```

      ```python
      plot_h(theta_1)
      
      ```

      ![](https://github.com/cowkite/cowkite.github.io/blob/master/_posts/assets/images/04.PNG?raw=true)

    - ë‹¤íšŒ ì‹œë„í•˜ì—¬ ìˆ˜ë ´í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸

      ```python
      costs = []
      iterations = []
      for i in range(20):
          y_hat = theta_1 * x
          residual = y_hat - y
      
          cost = (residual ** 2).sum()/(2*n)
      
          gradient = 1/n * (residual*x).sum()
      
          theta_1 -= alpha * gradient
          
          print('theta_1 :', theta_1, 'Cost :', cost, 'gradient :', gradient)
          plot_h(theta_1)
          plt.show()
          
          iterations.append(i)
          costs.append(cost)
      
      ```

      ![](https://github.com/cowkite/cowkite.github.io/blob/master/_posts/assets/images/05.PNG?raw=true)

      -> *(ì´ë¯¸ì§€ëŠ” ì˜ë ¸ì§€ë§Œ)* **theta ê°’ì´ ì ì  0.7ì— ê°€ê¹Œì›Œì§**

      ```python
      plt.plot(iterations, costs)
      
      ```

      ![](https://github.com/cowkite/cowkite.github.io/blob/master/_posts/assets/images/06.PNG?raw=true)

  

  ### 2-2. EDA

  - í†µê³„ì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì˜ˆì¸¡ì¹˜ê°€ ìˆëŠ”ì§€. ë°ì´í„°ë¥¼ ì‹œê°í™”í•˜ëŠ” ê³¼ì •.

  - ê²°êµ­ì€ Preprocessingì„ í•˜ê¸° ìœ„í•œ ê³¼ì •ì„

    #### 2-2-1. ë¬¸ì œ ì •ì˜

    - ì•„ì´ì˜¤ì™€ ì£¼ ì—ì„ìŠ¤(Ames)ì— ìˆëŠ” ì£¼ê±°ìš© ì£¼íƒì„ ì„¤ëª…í•˜ëŠ” 79ê°€ì§€ ë³€ìˆ˜ë¡œ ê° ì£¼íƒì˜ ìµœì¢… ê°€ê²©ì„ ì˜ˆì¸¡í•˜ê¸°

    #### 2-2-2. í›ˆë ¨, í…ŒìŠ¤íŠ¸ ë°ì´í„° í™•ì¸í•˜ê¸°

    - ë°ì´í„° í™•ì¸

      ```python
      import numpy as np # linear algebra
      import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
      %matplotlib inline
      import matplotlib.pyplot as plt  # Matlab-style plotting
      import seaborn as sns
      color = sns.color_palette()
      sns.set_style('darkgrid')
      import warnings
      def ignore_warn(*args, **kwargs):
          pass
      warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)
      
      
      from scipy import stats
      from scipy.stats import norm, skew #for some statistics
      
      
      pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points
      
      
      from subprocess import check_output
      
      #Now let's import and put the train and test datasets in  pandas dataframe
      train = pd.read_csv('./data/ames_house_prices/train.csv')
      test = pd.read_csv('./data/ames_house_prices/test.csv')
      
      ##display the first five rows of the train dataset.
      train.head(5)
      
      ```

      ![](https://github.com/cowkite/cowkite.github.io/blob/master/_posts/assets/images/07.PNG?raw=true)

      -> *í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ë ˆì´ë¸”ì´ ì—†ì–´ì„œ ë°ì´í„° ì‚¬ì´ì¦ˆê°€ 1 ì ìŒ*

  

  ### 2-3. Preprocessing

  - Preprocessing ê³¼ì •

    #### 2-3-1. Outliers (ì´ìƒì¹˜)

    - Training ê³¼ì •ì—ì„œ ì„±ëŠ¥ì— ì˜í–¥ì„ ì£¼ê¸° ë•Œë¬¸ì— ì œê±°í•´ì•¼í•¨

    - ë°ì´í„° ê´€ì¸¡

      ```python
      fig, ax = plt.subplots()
      ax.scatter(x = train['GrLivArea'], y = train['SalePrice'])
      plt.ylabel('SalePrice', fontsize=13)
      plt.xlabel('GrLivArea', fontsize=13)
      plt.show()
      
      ```

      ![](https://github.com/cowkite/cowkite.github.io/blob/master/_posts/assets/images/08.PNG?raw=true)

    - Training ë°ì´í„°ì˜ GrLivArea ì¤‘ ì˜¤ë¥¸ìª½ í•˜ë‹¨ì— ì´ìƒì¹˜(outlier) 2ê°œ ì‚­ì œ

      ```python
      #Deleting outliers
      train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)
      
      #Check the graphic again
      fig, ax = plt.subplots()
      ax.scatter(train['GrLivArea'], train['SalePrice'])
      plt.ylabel('SalePrice', fontsize=13)
      plt.xlabel('GrLivArea', fontsize=13)
      plt.show()
      
      ```

      ![](https://github.com/cowkite/cowkite.github.io/blob/master/_posts/assets/images/09.PNG?raw=true)

    #### 2-3-2. Target Variable

    - ìš°í¸í–¥ ë˜ì–´ìˆëŠ” ë³€ìˆ˜ ì¡°ì •

      ```python
      #We use the numpy fuction log1p which  applies log(1+x) to all elements of the column
      train["SalePrice"] = np.log1p(train["SalePrice"])
      
      #Check the new distribution 
      sns.distplot(train['SalePrice'] , fit=norm);
      
      # Get the fitted parameters used by the function
      (mu, sigma) = norm.fit(train['SalePrice'])
      print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))
      
      #Now plot the distribution
      plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
                  loc='best')
      plt.ylabel('Frequency')
      plt.title('SalePrice distribution')
      
      ```

      

    #### 2-3-3. Features Engineering

    - **Missing Data**

      1. **Missing ratio í™•ì¸**

         ```python
         all_data_na = (all_data.isnull().sum() / len(all_data)) * 100
         all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]
         missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})
         missing_data.head(20)
         
         ```

         ![](https://github.com/cowkite/cowkite.github.io/blob/master/_posts/assets/images/10.PNG?raw=true)

         -> 99%ì˜ missing ratioë¥¼ ê°€ì§„ í•­ëª©ì€ ì˜ë¯¸ê°€ ì—†ëŠ” ë°ì´í„°ê¸° ë•Œë¬¸ì—, íŠ¹ì • ë¯¸ì‹±ë¥ ì„ ê°€ì§€ëŠ” ë°ì´í„°ëŠ” ì‚­ì œê°€ í•„ìš”í•˜ë‹¤.

      2. **Data Correlation**

         ì¢…ì† ë³€ìˆ˜ê°„ì˜ ì–´ë–¤ relation shipì´ ìˆëŠ”ì§€ í™•ì¸

         ```python
         #Correlation map to see how features are correlated with SalePrice
         corrmat = train.corr()
         plt.subplots(figsize=(12,9))
         sns.heatmap(corrmat, vmax=0.9, square=True)
         
         ```

         ![](https://github.com/cowkite/cowkite.github.io/blob/master/_posts/assets/images/11.PNG?raw=true)

         -> ê´€ë ¨ì´ ìˆìœ¼ë©´ ë°ì€ìƒ‰, ê´€ë ¨ì´ ì—†ìœ¼ë©´ ì–´ë‘ìš´ìƒ‰.

         -> í°ìƒ‰ì˜ ëŒ€ê°ì„ ì€ ë³¸ì¸ ë°ì´í„°ê¸° ë•Œë¬¸ì„

      3. **Missing values** ì²˜ë¦¬í•˜ê¸°

         - ì „ì²´ ë°ì´í„°ë¥¼ `N/A`ë¡œ ì²˜ë¦¬í•˜ê¸°

           ```python
           all_data["PoolQC"] = all_data["PoolQC"].fillna("None")
           
           ```

         - ì¤‘ê°„ ê°’ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸°

           ```python
           all_data["LotFrontage"].fillna(all_data.groupby("Neighborhood")["LotFrontage"].transform("median"), inplace=True)
           
           ```

         - ë°ì´í„° ì œê±°í•˜ê¸° (ë°ì´í„°ì˜ ì˜ë¯¸ê°€ ì—†ëŠ” ê²½ìš°)

           ```python
           all_data = all_data.drop(['Utilities'], axis=1)
           
           ```

         - íŠ¹ì • ì—´ì˜ ê°’ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° (ê²°ì¸¡ì¹˜ê°€ ì ì€ ê²½ìš°)

           ```python
           all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])
           
           ```

    - **Transforming some numerical variables to categorical**

      ```python
      #MSSubClass=The building class
      all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)
      
      
      #Changing OverallCond into a categorical variable
      all_data['OverallCond'] = all_data['OverallCond'].astype(str)
      
      
      #Year and month sold are transformed into categorical features.
      all_data['YrSold'] = all_data['YrSold'].astype(str)
      all_data['MoSold'] = all_data['MoSold'].astype(str)
      
      ```

    - **Transforming some categorical variables to numerical**

      ```python
      from sklearn.preprocessing import LabelEncoder
      cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', 
              'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', 
              'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',
              'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', 
              'YrSold', 'MoSold')
      # process columns, apply LabelEncoder to categorical features
      for c in cols:
          lbl = LabelEncoder() 
          lbl.fit(list(all_data[c].values)) 
          all_data[c] = lbl.transform(list(all_data[c].values))
      
      # shape        
      print('Shape all_data: {}'.format(all_data.shape))
      
      ```

    - **Normalizeing skewd features**

      ```python
      skewness = skewness[abs(skewness) > 0.75]
      print("There are {} skewed numerical features to Box Cox transform".format(skewness.shape[0]))
      
      from scipy.special import boxcox1p
      skewed_features = skewness.index
      lam = 0.15
      for feat in skewed_features:
          #all_data[feat] += 1
          all_data[feat] = boxcox1p(all_data[feat], lam)
          
      #all_data[skewed_features] = np.log1p(all_data[skewed_features])
      
      ```

      -> *Box-Cox Transformation of (highly) skewed features* (http://onlinestatbook.com/2/transformations/box-cox.html)https://datascienceschool.net/view-notebook/3f485c426a4b49fc9de95a02137ca6b4/)

    - **Scaling features**

    - **Add/Drop more features**

      - Check correlation.
      - Adding one more important feature.
      - Getting dummy categorical features.
      - Getting the new train and test sets.

      

  ### 2-4. Model Selection

  - ì–´ë ¤ìš´.. ê³¼ì •ì´.. ì•„ë‹ˆëë‹ˆë‹¤.. import ë°›ì€ ê²ƒ í˜¸ì¶œë§Œ í•˜ë©´ ë¨

    #### 2-4-1. Import libratries

    - í•„ìš”í•œ í•­ëª©ë“¤ import

      ```python
      from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC
      from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor
      from sklearn.kernel_ridge import KernelRidge
      from sklearn.pipeline import make_pipeline
      from sklearn.preprocessing import RobustScaler
      from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone
      from sklearn.model_selection import KFold, cross_val_score, train_test_split
      from sklearn.metrics import mean_squared_error
      import xgboost as xgb
      import lightgbm as lgb
      
      ```

    #### 2-4-2. Define a cross validation strategy

    - í›¨ì”¬ ì„±ëŠ¥ì´ ì¢‹ì•„ì§

    - `rms`: ì„±ëŠ¥ì„ ì¸¡ì •í•˜ëŠ”ë° ê°€ì¥ ë§ì´ ì‚¬ìš©. ì—¬ê¸°ì„œëŠ” cross validationì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•œë‹¤.

      ```python
      #Validation function
      n_folds = 5
      
      def rmsle_cv(model):
          kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)
          rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring="neg_mean_squared_error", cv = kf))
          return(rmse)
      
      ```

    #### 2-4-3. Making Base Regression Model

    - **LASSO Regression**

      ```python
      lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))
      
      ```

    - **Eleastic Net Regression**

      ```python
      ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))
      
      ```

    - *(ì´í•˜ ìƒëµ)*

      

  ### 2-5. Model Evaluation & Prediction

  - ë°ì´í„° ì„±ëŠ¥ í–¥ìƒ ì‹œí‚¤ê¸°

    #### 2-5-1. Base model Evaluation

    - **LASSO Regression**

      ```python
      score = rmsle_cv(lasso)
      print("\nLasso score: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))
      
      ```

      *-> Lasso score: 0.1172 (0.0076)*

    - **Eleastic Net Regression**

      ```python
      score = rmsle_cv(ENet)
      print("ElasticNet score: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))
      
      ```

      *-> ElasticNet score: 0.1172 (0.0077)*

    - *(ì´í•˜ ìƒëµ)*

    #### 2-5-2. Emsembling

    - *ì•™ìƒë¸”(Ensemble) í•™ìŠµì€ ì—¬ëŸ¬ ê°œì˜ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ê³ , ê·¸ ì˜ˆì¸¡ì„ ê²°í•©í•¨ìœ¼ë¡œì¨ ë³´ë‹¤ ì •í™•í•œ ìµœì¢… ì˜ˆì¸¡ì„ ë„ì¶œí•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. í•˜ë‚˜ì˜ ê°•í•œ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ ì—¬ëŸ¬ ê°œì˜ ì•½í•œ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì´ ë‚«ë‹¤ ë¼ëŠ” ì•„ì´ë””ì–´ë¥¼ ê°€ì§€ê³  ì´í•´í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€, ì˜ìƒ, ìŒì„± ë“±ì˜ ë¹„ì •í˜• ë°ì´í„°ì˜ ë¶„ë¥˜ëŠ” ë”¥ëŸ¬ë‹ì´ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆì§€ë§Œ, ëŒ€ë¶€ë¶„ì˜ ì •í˜• ë°ì´í„° ë¶„ë¥˜ ì‹œì—ëŠ” ì•™ìƒë¸”ì´ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚´ê³  ìˆìŠµë‹ˆë‹¤. ë¬¸ì œì™€ ë°ì´í„°ì— ë”°ë¼ ë‹¨ì¼ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë” ì¢‹ì€ ê²½ìš°ë„ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì•™ìƒë¸” ê¸°ë²•ì„ ì‚¬ìš©í•˜ë©´ ë” ìœ ì—°ì„±ìˆëŠ” ëª¨ë¸ì„ ë§Œë“¤ë©° ë” ì¢‹ì€ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Regressionì—ì„œ ì•™ìƒë¸” í•™ìŠµì˜ ìœ í˜•ì€ ê°€ì¥ ë§ì´ ì•Œë ¤ì§„ Averaging, Stacking ë“±ì´ ìˆìœ¼ë©°, ê·¸ ì™¸ì—ë„ ë‹¤ì–‘í•œ ì•™ìƒë¸” í•™ìŠµì˜ ìœ í˜•ì´ ì—°êµ¬ë˜ê³  ìˆìŠµë‹ˆë‹¤.*

    - ì‹¤ì œ ì„œë¹„ìŠ¤í•  ë•ŒëŠ” ì˜ë¯¸ ì—†ëŠ” (0.0001%) ì„±ëŠ¥ ì°¨ì´ì„. *...ê°•ì‚¬ë‹˜ í”¼ì…œ kaggel ëŒ€íšŒìš©ìœ¼ë¡œë§Œ ì‚¬ìš©í• ë“¯í•œë‹¤ê³ í•¨...*

    - **Averaging**

      *ì—ë²„ë¦¬ì§•(Averaging)ì€ ê°€ì¥ ì‰¬ìš´ ì•™ìƒë¸” ê¸°ë²•ì…ë‹ˆë‹¤. ì„œë¡œ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ì„ ê°€ì§„ Regression ëª¨ë¸ë¥¼ ê²°í•©í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.*

      1. *Simple Averaging*

         *íšŒê·€ ë¬¸ì œì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, ê° ì˜ˆì¸¡ê°’ì„ í‰ê· ë‚´ì–´ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ê²½ìš°ì— ë”°ë¼ ê³¼ëŒ€ì í•©ì„ ì¤„ì—¬ì£¼ê³ , ë” ë¶€ë“œëŸ¬ìš´ íšŒê·€ëª¨ë¸ì„ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.*

      2. *Weighted Averaging*

         *ìœ„ì—ì„œ í‰ê· ì„ ë‚¼ ë•Œ, ê° ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜ë¥¼ ë‘ì–´ í‰ê· ë‚´ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.*

    - **Stacking**

      *In this approach, we add a meta-model on averaged base models and use the out-of-folds predictions of these base models to train our meta-model.*

      *The procedure, for the training part, may be described as follows:*

      *Split the total training set into two disjoint sets (here train and .holdout )*

      *Train several base models on the first part (train)*

      *Test these base models on the second part (holdout)*

      *Use the predictions from 3) (called out-of-folds predictions) as the inputs, and the correct responses (target variable) as the outputs to train a higher level learner called meta-model.*

      *The first three steps are done iteratively . If we take for example a 5-fold stacking , we first split the training data into 5 folds. Then we will do 5 iterations. In each iteration, we train every base model on 4 folds and predict on the remaining fold (holdout fold).*

      *So, we will be sure, after 5 iterations , that the entire data is used to get out-of-folds predictions that we will then use as new feature to train our meta-model in the step 4.*

      *For the prediction part , We average the predictions of all base models on the test data and used them as meta-features on which, the final prediction is done with the meta-model.*

    #### 2-5-3. Prediction

    - Ensemble prediction
    - Making Prediction Result into CSV File
